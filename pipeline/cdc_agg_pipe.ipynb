{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get all xpt file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpt_file_list = glob.glob('./xpt_data/*.XPT')\n",
    "xpt_data_abbr_list = [os.path.basename(fn).split('.')[0] for fn in xpt_file_list]\n",
    "xpt_data_abbr_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get variable names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### individual page codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_desc(table_abbr):\n",
    "    # get the html page with the variable codebook for all xpt files\n",
    "    # use this to rename the columns to descriptive cols\n",
    "    # table_abbr = 'P_ACQ'\n",
    "\n",
    "    var_code_list = []\n",
    "    var_desc_list = []\n",
    "    try:\n",
    "        url = f'https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/{table_abbr}.htm'\n",
    "        page = requests.get(url)\n",
    "\n",
    "        # beautiful soup is good for parsing html\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # this is the id for the table on the variable codebook page\n",
    "        tbl = soup.find(\"div\",{\"id\":\"Codebook\"})\n",
    "\n",
    "\n",
    "        for var_table in tbl.findAll('div', {'class':'pagebreak'}):\n",
    "            var_code = var_table.find('dt', string=\"Variable Name: \").find_next('dd')\n",
    "            sas_title = var_table.find('dt', string=\"SAS Label: \")\n",
    "            \n",
    "            # if no sas label is found, use next dd\n",
    "            # seems like this happens with check boxes\n",
    "            if sas_title is None:\n",
    "                var_desc = var_code.find_next('dd')\n",
    "            # otherwise use the next dd element\n",
    "            else:\n",
    "                var_desc = sas_title.find_next('dd')\n",
    "            \n",
    "            var_code_list.append(var_code.text)\n",
    "            var_desc_list.append(var_desc.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    # check to make sure same number of var codes and desc\n",
    "    # before zipping\n",
    "    if len(var_code_list) != len(var_desc_list):\n",
    "        print(f'error on file {table_abbr}')\n",
    "        print(var_code_list)\n",
    "        print(var_desc_list)\n",
    "        print(len(var_code_list))\n",
    "        print(len(var_desc_list))\n",
    "        raise Exception ('length mismatch on var code list and var desc list')\n",
    "        \n",
    "\n",
    "    ind_var_map = dict(zip(var_code_list, var_desc_list))\n",
    "    return ind_var_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_map = {}\n",
    "\n",
    "for abbr in tqdm(xpt_data_abbr_list):\n",
    "    # merge dicts as we go\n",
    "    var_map.update(get_var_desc(abbr))\n",
    "var_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entire variable codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the html page with the variable codebook for all xpt files\n",
    "# # use this to rename the columns to descriptive cols\n",
    "# url = 'https://wwwn.cdc.gov/nchs/nhanes/search/variablelist.aspx?Component=Questionnaire&Cycle=2017-2020'\n",
    "# page = requests.get(url)\n",
    "\n",
    "# # beautiful soup is good for parsing html\n",
    "# soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# # this is the id for the table on the variable codebook bage\n",
    "# tbl = soup.find(\"table\",{\"id\":\"GridView1\"})\n",
    "\n",
    "# # get the table and load into a df\n",
    "# df_var_mapping = pd.read_html(str(tbl))[0]\n",
    "# df_var_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a key value mapping between the variable name and description\n",
    "# var_map = dict(zip(\n",
    "#     df_var_mapping['Variable Name'], \n",
    "#     df_var_mapping['Variable Description']\n",
    "# ))\n",
    "\n",
    "# var_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read xpt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_xpt_files(folder_path):\n",
    "    \"\"\"Input folder path to read multiple xpt files in folder\n",
    "       Returns dictionary with key: file name, \n",
    "                               value: df\"\"\"\n",
    "    df_dict = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if 'XPT' in os.path.splitext(file_name)[1]:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_sas(file_path, format='xport')\n",
    "            name = os.path.splitext(file_name)[0]\n",
    "            if df.empty:\n",
    "                raise Exception (f'Empty dataframe from file: {name}')\n",
    "            df_dict[name] = df\n",
    "        else:\n",
    "            print(f'not loading file {file_name}')\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "\n",
    "def full_outer_join(dataframes):\n",
    "    joined_df = None\n",
    "    for df in dataframes.values():\n",
    "        if joined_df is None:\n",
    "            joined_df = df\n",
    "        else:\n",
    "            joined_df = pd.merge(joined_df, df, on='SEQN', how='outer')\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "folder_path = './xpt_data/'\n",
    "dataframes_dict = read_xpt_files(folder_path)\n",
    "df_cdc_joined = full_outer_join(dataframes_dict)\n",
    "df_cdc_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdc_joined.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert bytes and clean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bytes and strip whitespace\n",
    "df_cdc_joined_clean = df_cdc_joined.apply(lambda x: x.str.decode('utf-8').str.strip() if x.dtype == \"object\" else x)\n",
    "# replace empty strings with nan\n",
    "df_cdc_joined_clean = df_cdc_joined_clean.replace('', np.nan)\n",
    "df_cdc_joined_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter to columns we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = '''SEQN\n",
    "FSD652CW\n",
    "HUQ010\n",
    "HUQ030\n",
    "HUQ090\n",
    "DPQ010\n",
    "DPQ020\n",
    "DPQ030\n",
    "DPQ040\n",
    "DPQ050\n",
    "DPQ060\n",
    "DPQ070\n",
    "DPQ080\n",
    "DPQ090\n",
    "DPQ100\n",
    "RXDUSE\n",
    "RXDDAYS\n",
    "RXDRSC1\n",
    "RXDRSC2\n",
    "RXDRSC3\n",
    "RXDRSD1\n",
    "RXDRSD2\n",
    "RXDRSD3\n",
    "RHQ074\n",
    "RHQ076\n",
    "RHD167\n",
    "RHQ171'''.split()\n",
    "cols_to_keep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rename based on the mapping obtained from cdc codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdc_joined_clean_trim = df_cdc_joined_clean[cols_to_keep]\n",
    "df_cdc_joined_clean_trim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rename columns with var descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cdc_joined_clean_trim.rename(columns=var_map, inplace=True)\n",
    "df_cdc_joined_clean_trim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick check on stats of selected cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the pandas display options using with statement\n",
    "# lets us supress sci notation and see all columns \n",
    "# without messing up your pandas output everywhere\n",
    "with pd.option_context('display.max_columns', None, 'display.float_format', lambda x: '%.3f' % x): # also can do all rows using 'display.max_rows', None, \n",
    "    display(df_cdc_joined_clean_trim.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01ff409f73b03d6519eeb1957ccfce57d615f2ffd1f1b7ba6202370b58e38602"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
