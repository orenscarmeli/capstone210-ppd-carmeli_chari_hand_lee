{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/jonhand/Desktop/w210_Capstone/capstone210-ppd-carmeli_chari_hand_lee/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from operator import mod\n",
    "from os import getcwd\n",
    "from os.path import exists, join\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import random\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import pickle\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SMQ681</th>\n",
       "      <th>SMQ690A</th>\n",
       "      <th>SMQ710</th>\n",
       "      <th>SMQ720</th>\n",
       "      <th>SMQ725</th>\n",
       "      <th>SMQ690B</th>\n",
       "      <th>SMQ740</th>\n",
       "      <th>SMQ690C</th>\n",
       "      <th>SMQ770</th>\n",
       "      <th>...</th>\n",
       "      <th>live_birth_count</th>\n",
       "      <th>age_at_first_birth</th>\n",
       "      <th>age_at_last_birth</th>\n",
       "      <th>months_since_birth</th>\n",
       "      <th>horomones_not_bc</th>\n",
       "      <th>smoked_100_cigs</th>\n",
       "      <th>currently_smoke</th>\n",
       "      <th>height_in</th>\n",
       "      <th>weight_lbs</th>\n",
       "      <th>attempt_weight_loss_1yr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109264</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109266</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109271</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109274</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35701</th>\n",
       "      <td>83726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35702</th>\n",
       "      <td>83727</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35703</th>\n",
       "      <td>83728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35704</th>\n",
       "      <td>83730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35705</th>\n",
       "      <td>83731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35706 rows × 863 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SEQN  SMQ681  SMQ690A  SMQ710  SMQ720  SMQ725  SMQ690B  SMQ740  \\\n",
       "0      109264     2.0      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "1      109266     2.0      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "2      109271     1.0      1.0     5.0    20.0     1.0      NaN     NaN   \n",
       "3      109273     1.0      1.0     5.0    13.0     1.0      NaN     NaN   \n",
       "4      109274     2.0      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "...       ...     ...      ...     ...     ...     ...      ...     ...   \n",
       "35701   83726     NaN      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "35702   83727     2.0      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "35703   83728     NaN      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "35704   83730     NaN      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "35705   83731     NaN      NaN     NaN     NaN     NaN      NaN     NaN   \n",
       "\n",
       "       SMQ690C  SMQ770  ...  live_birth_count  age_at_first_birth  \\\n",
       "0          NaN     NaN  ...               NaN                 NaN   \n",
       "1          NaN     NaN  ...               NaN                 NaN   \n",
       "2          NaN     NaN  ...               NaN                 NaN   \n",
       "3          NaN     NaN  ...               NaN                 NaN   \n",
       "4          NaN     NaN  ...               NaN                 NaN   \n",
       "...        ...     ...  ...               ...                 ...   \n",
       "35701      NaN     NaN  ...               NaN                 NaN   \n",
       "35702      NaN     NaN  ...               NaN                 NaN   \n",
       "35703      NaN     NaN  ...               NaN                 NaN   \n",
       "35704      NaN     NaN  ...               NaN                 NaN   \n",
       "35705      NaN     NaN  ...               NaN                 NaN   \n",
       "\n",
       "       age_at_last_birth  months_since_birth  horomones_not_bc  \\\n",
       "0                    NaN                 NaN               NaN   \n",
       "1                    NaN                 NaN               2.0   \n",
       "2                    NaN                 NaN               NaN   \n",
       "3                    NaN                 NaN               NaN   \n",
       "4                    NaN                 NaN               NaN   \n",
       "...                  ...                 ...               ...   \n",
       "35701                NaN                 NaN               NaN   \n",
       "35702                NaN                 NaN               NaN   \n",
       "35703                NaN                 NaN               NaN   \n",
       "35704                NaN                 NaN               NaN   \n",
       "35705                NaN                 NaN               NaN   \n",
       "\n",
       "       smoked_100_cigs  currently_smoke  height_in  weight_lbs  \\\n",
       "0                  NaN              NaN        NaN         NaN   \n",
       "1                  2.0              NaN       64.0       210.0   \n",
       "2                  1.0              1.0       72.0       222.0   \n",
       "3                  1.0              1.0       72.0       165.0   \n",
       "4                  2.0              NaN       75.0       219.0   \n",
       "...                ...              ...        ...         ...   \n",
       "35701              2.0              NaN       66.0       180.0   \n",
       "35702              2.0              NaN       69.0       150.0   \n",
       "35703              NaN              NaN        NaN         NaN   \n",
       "35704              NaN              NaN        NaN         NaN   \n",
       "35705              NaN              NaN        NaN         NaN   \n",
       "\n",
       "       attempt_weight_loss_1yr  \n",
       "0                          NaN  \n",
       "1                          1.0  \n",
       "2                          2.0  \n",
       "3                          2.0  \n",
       "4                          NaN  \n",
       "...                        ...  \n",
       "35701                      2.0  \n",
       "35702                      2.0  \n",
       "35703                      NaN  \n",
       "35704                      NaN  \n",
       "35705                      NaN  \n",
       "\n",
       "[35706 rows x 863 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_cdc_clean = pd.read_csv('../data/cdc_nhanes_survey_responses_clean.csv')\n",
    "# v2 csv\n",
    "df_cdc_clean = pd.read_csv('../data/cdc_nhanes_survey_responses_clean.csv')\n",
    "\n",
    "# filter to moms\n",
    "# df_cdc_clean = df_cdc_clean[df_cdc_clean['has_been_pregnant'] == 1]\n",
    "# df_cdc_clean = df_cdc_clean.drop(columns=['has_been_pregnant'])\n",
    "\n",
    "df_cdc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SEQN',\n",
       " 'MDD',\n",
       " 'little_interest_in_doing_things',\n",
       " 'feeling_down_depressed_hopeless',\n",
       " 'trouble_falling_or_staying_asleep',\n",
       " 'feeling_tired_or_having_little_energy',\n",
       " 'poor_appetitie_or_overeating',\n",
       " 'trouble_concentrating',\n",
       " 'moving_or_speaking_to_slowly_or_fast',\n",
       " 'difficult_doing_daytoday_tasks',\n",
       " 'alcoholic_drinks_past_12mo',\n",
       " 'count_days_seen_doctor_12mo',\n",
       " 'count_days_moderate_recreational_activity',\n",
       " 'count_minutes_moderate_recreational_activity',\n",
       " 'count_minutes_moderate_sedentary_activity',\n",
       " 'count_lost_10plus_pounds',\n",
       " 'food_security_level_household',\n",
       " 'food_security_level_adult',\n",
       " 'general_health_condition',\n",
       " 'duration_last_healthcare_visit',\n",
       " 'monthly_poverty_index_category',\n",
       " 'count_hours_worked_last_week',\n",
       " 'education_level',\n",
       " 'has_health_insurance',\n",
       " 'has_health_insurance_gap',\n",
       " 'has_smoked_tabacco_last_5days',\n",
       " 'is_male',\n",
       " 'is_usa_born',\n",
       " 'has_diabetes',\n",
       " 'has_overweight_diagnosis',\n",
       " 'has_tried_to_lose_weight_12mo',\n",
       " 'has_been_pregnant',\n",
       " 'monthly_poverty_index',\n",
       " 'age_with_angina_pectoris',\n",
       " 'age_liver_condition',\n",
       " 'age_range_first_menstrual_period',\n",
       " 'annual_healthcare_visit_count',\n",
       " 'have_liver_condition',\n",
       " 'type_of_work_done_last_week',\n",
       " 'weight_change_intentional',\n",
       " 'days_nicotine_substitute_used',\n",
       " 'pain_relief_from_cardio_recoverytime',\n",
       " 'drank_alc',\n",
       " 'alc_drinking_freq',\n",
       " 'alc_per_day',\n",
       " 'times_with_4or5_alc',\n",
       " 'times_with_8plus_alc',\n",
       " 'times_with_12plus_alc',\n",
       " '4plus_alc_daily',\n",
       " 'days_4plus_drinks_occasion',\n",
       " 'high_bp',\n",
       " 'age_hypertension',\n",
       " 'hypertension_prescription',\n",
       " 'high_bp_prescription',\n",
       " 'high_cholesterol',\n",
       " 'cholesterol_prescription',\n",
       " 'chest_discomfort',\n",
       " 'milk_consumption_freq',\n",
       " 'govmnt_meal_delivery',\n",
       " 'nonhomemade_meals',\n",
       " 'fastfood_meals',\n",
       " 'readytoeat_meals',\n",
       " 'frozen_pizza',\n",
       " 'emergency_food_received',\n",
       " 'food_stamps_used',\n",
       " 'wic_benefit_used',\n",
       " 'general_health',\n",
       " 'regular_healthcare_place',\n",
       " 'time_since_last_healthcare',\n",
       " 'overnight_in_hospital',\n",
       " 'seen_mental_health_professional',\n",
       " 'have_health_insurance',\n",
       " 'have_private_insurance',\n",
       " 'plan_cover_prescriptions',\n",
       " 'family_poverty_level',\n",
       " 'family_poverty_level_category',\n",
       " 'anemia_treatment',\n",
       " 'blood_transfusion',\n",
       " 'heart_failure',\n",
       " 'coronary_heart_disease',\n",
       " 'angina_pectoris',\n",
       " 'heart_attack',\n",
       " 'thyroid_issues',\n",
       " 'respiratory_issues',\n",
       " 'abdominal_pain',\n",
       " 'gallbladder_surgery',\n",
       " 'dr_recommend_lose_weight',\n",
       " 'dr_recommend_exercise',\n",
       " 'dr_recommend_reduce_salt',\n",
       " 'dr_recommend_reduce_fat',\n",
       " 'currently_losing_weight',\n",
       " 'currently_increase_exercise',\n",
       " 'currently_reducing_salt',\n",
       " 'currently_reducing_fat',\n",
       " 'metal_objects',\n",
       " 'hours_worked',\n",
       " 'over_35_hrs_worked',\n",
       " 'work_schedule',\n",
       " 'vigorous_work',\n",
       " 'walk_or_bicycle',\n",
       " 'vigorous_recreation',\n",
       " 'moderate_recreation',\n",
       " 'regular_periods',\n",
       " 'age_last_period',\n",
       " 'try_pregnancy_1yr',\n",
       " 'see_dr_fertility',\n",
       " 'pelvic_infection',\n",
       " 'pregnant_now',\n",
       " 'pregnancy_count',\n",
       " 'diabetes_pregnancy',\n",
       " 'delivery_count',\n",
       " 'live_birth_count',\n",
       " 'age_at_first_birth',\n",
       " 'age_at_last_birth',\n",
       " 'months_since_birth',\n",
       " 'horomones_not_bc',\n",
       " 'smoked_100_cigs',\n",
       " 'currently_smoke',\n",
       " 'height_in',\n",
       " 'weight_lbs',\n",
       " 'attempt_weight_loss_1yr']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cols_to_keep = ['SEQN', 'MDD']\n",
    "# cols_to_keep.extend(df_cdc_clean.columns.tolist()[-38:])\n",
    "cols_to_keep.extend([col for col in df_cdc_clean.columns.tolist() if '_' in col and '_x' not in col and '_y' not in col])\n",
    "cols_to_keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cols_to_keep = [\n",
    "    'SEQN',\n",
    "    'MDD',\n",
    "    # 'is_male',\n",
    "    # 'has_been_pregnant',\n",
    "    'age_with_angina_pectoris',\n",
    "    'age_liver_condition',\n",
    "    'age_range_first_menstrual_period',\n",
    "    'annual_healthcare_visit_count',\n",
    "    'have_liver_condition',\n",
    "    'type_of_work_done_last_week',\n",
    "    'weight_change_intentional',\n",
    "    'days_nicotine_substitute_used',\n",
    "    'pain_relief_from_cardio_recoverytime',\n",
    "    # Depression screener\n",
    "    'little_interest_in_doing_things',\n",
    "    'feeling_down_depressed_hopeless',\n",
    "    'trouble_falling_or_staying_asleep',\n",
    "    'feeling_tired_or_having_little_energy',\n",
    "    'poor_appetitie_or_overeating',\n",
    "    'feeling_bad_about_yourself',\n",
    "    'trouble_concentrating',\n",
    "    'moving_or_speaking_to_slowly_or_fast',\n",
    "    'thoughts_you_would_be_better_off_dead',\n",
    "    'difficult_doing_daytoday_tasks',\n",
    "    # Alcohol & smoking\n",
    "    'has_smoked_tabacco_last_5days',\n",
    "    'alcoholic_drinks_past_12mo',    \n",
    "    # Diet & Nutrition\n",
    "    'how_healthy_is_your_diet',    \n",
    "    'count_lost_10plus_pounds',\n",
    "    'has_tried_to_lose_weight_12mo',       \n",
    "    # Physical health & Medical History\n",
    "    'count_days_seen_doctor_12mo',\n",
    "    'duration_last_healthcare_visit',        \n",
    "    'count_days_moderate_recreational_activity',   \n",
    "    'count_minutes_moderate_recreational_activity',\n",
    "    'count_minutes_moderate_sedentary_activity',\n",
    "    'general_health_condition',    \n",
    "    'has_diabetes',\n",
    "    'has_overweight_diagnosis',         \n",
    "    # Demographic data\n",
    "    'food_security_level_household',   \n",
    "    'food_security_level_adult',    \n",
    "    'monthly_poverty_index_category',\n",
    "    'monthly_poverty_index',\n",
    "    'count_hours_worked_last_week',\n",
    "    'age_in_years',   \n",
    "    'education_level',\n",
    "    'is_usa_born',    \n",
    "    'has_health_insurance',\n",
    "    'has_health_insurance_gap'   \n",
    "]\n",
    "len(cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>MDD</th>\n",
       "      <th>annual_healthcare_visit_count</th>\n",
       "      <th>type_of_work_done_last_week</th>\n",
       "      <th>little_interest_in_doing_things</th>\n",
       "      <th>feeling_down_depressed_hopeless</th>\n",
       "      <th>trouble_falling_or_staying_asleep</th>\n",
       "      <th>feeling_tired_or_having_little_energy</th>\n",
       "      <th>poor_appetitie_or_overeating</th>\n",
       "      <th>feeling_bad_about_yourself</th>\n",
       "      <th>...</th>\n",
       "      <th>has_overweight_diagnosis</th>\n",
       "      <th>food_security_level_household</th>\n",
       "      <th>food_security_level_adult</th>\n",
       "      <th>monthly_poverty_index_category</th>\n",
       "      <th>monthly_poverty_index</th>\n",
       "      <th>age_in_years</th>\n",
       "      <th>education_level</th>\n",
       "      <th>is_usa_born</th>\n",
       "      <th>has_health_insurance</th>\n",
       "      <th>has_health_insurance_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109264</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109266</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109271</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109273</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109274</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35701</th>\n",
       "      <td>83726</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35702</th>\n",
       "      <td>83727</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35703</th>\n",
       "      <td>83728</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35704</th>\n",
       "      <td>83730</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35705</th>\n",
       "      <td>83731</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35706 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SEQN  MDD  annual_healthcare_visit_count  \\\n",
       "0      109264    0                            0.0   \n",
       "1      109266    0                            1.0   \n",
       "2      109271    0                            2.0   \n",
       "3      109273    0                            3.0   \n",
       "4      109274    0                            0.0   \n",
       "...       ...  ...                            ...   \n",
       "35701   83726    0                            2.0   \n",
       "35702   83727    0                            1.0   \n",
       "35703   83728    0                            0.0   \n",
       "35704   83730    0                            0.0   \n",
       "35705   83731    0                            1.0   \n",
       "\n",
       "       type_of_work_done_last_week  little_interest_in_doing_things  \\\n",
       "0                              NaN                              NaN   \n",
       "1                              1.0                              0.0   \n",
       "2                              4.0                              2.0   \n",
       "3                              1.0                              2.0   \n",
       "4                              4.0                              0.0   \n",
       "...                            ...                              ...   \n",
       "35701                          1.0                              NaN   \n",
       "35702                          1.0                              0.0   \n",
       "35703                          NaN                              NaN   \n",
       "35704                          NaN                              NaN   \n",
       "35705                          NaN                              NaN   \n",
       "\n",
       "       feeling_down_depressed_hopeless  trouble_falling_or_staying_asleep  \\\n",
       "0                                  NaN                                NaN   \n",
       "1                                  0.0                                0.0   \n",
       "2                                  1.0                                0.0   \n",
       "3                                  2.0                                2.0   \n",
       "4                                  0.0                                0.0   \n",
       "...                                ...                                ...   \n",
       "35701                              NaN                                NaN   \n",
       "35702                              0.0                                0.0   \n",
       "35703                              NaN                                NaN   \n",
       "35704                              NaN                                NaN   \n",
       "35705                              NaN                                NaN   \n",
       "\n",
       "       feeling_tired_or_having_little_energy  poor_appetitie_or_overeating  \\\n",
       "0                                        NaN                           NaN   \n",
       "1                                        0.0                           0.0   \n",
       "2                                        0.0                           0.0   \n",
       "3                                        2.0                           2.0   \n",
       "4                                        0.0                           0.0   \n",
       "...                                      ...                           ...   \n",
       "35701                                    NaN                           NaN   \n",
       "35702                                    0.0                           0.0   \n",
       "35703                                    NaN                           NaN   \n",
       "35704                                    NaN                           NaN   \n",
       "35705                                    NaN                           NaN   \n",
       "\n",
       "       feeling_bad_about_yourself  ...  has_overweight_diagnosis  \\\n",
       "0                             NaN  ...                       NaN   \n",
       "1                             0.0  ...                       1.0   \n",
       "2                             0.0  ...                       0.0   \n",
       "3                             2.0  ...                       1.0   \n",
       "4                             0.0  ...                       1.0   \n",
       "...                           ...  ...                       ...   \n",
       "35701                         NaN  ...                       0.0   \n",
       "35702                         0.0  ...                       0.0   \n",
       "35703                         NaN  ...                       NaN   \n",
       "35704                         NaN  ...                       NaN   \n",
       "35705                         NaN  ...                       NaN   \n",
       "\n",
       "       food_security_level_household  food_security_level_adult  \\\n",
       "0                                1.0                        1.0   \n",
       "1                                1.0                        1.0   \n",
       "2                                3.0                        3.0   \n",
       "3                                4.0                        4.0   \n",
       "4                                3.0                        3.0   \n",
       "...                              ...                        ...   \n",
       "35701                            1.0                        1.0   \n",
       "35702                            1.0                        1.0   \n",
       "35703                            1.0                        1.0   \n",
       "35704                            2.0                        2.0   \n",
       "35705                            1.0                        1.0   \n",
       "\n",
       "       monthly_poverty_index_category  monthly_poverty_index  age_in_years  \\\n",
       "0                                 1.0                    1.0            13   \n",
       "1                                 3.0                    5.0            29   \n",
       "2                                 1.0                    1.0            49   \n",
       "3                                 1.0                    0.0            36   \n",
       "4                                 1.0                    1.0            68   \n",
       "...                               ...                    ...           ...   \n",
       "35701                             3.0                    2.0            40   \n",
       "35702                             NaN                    NaN            26   \n",
       "35703                             3.0                    5.0             2   \n",
       "35704                             1.0                    0.0             7   \n",
       "35705                             3.0                    5.0            11   \n",
       "\n",
       "       education_level  is_usa_born  has_health_insurance  \\\n",
       "0                  NaN          1.0                   1.0   \n",
       "1                  5.0          0.0                   1.0   \n",
       "2                  2.0          1.0                   1.0   \n",
       "3                  4.0          1.0                   1.0   \n",
       "4                  4.0          1.0                   1.0   \n",
       "...                ...          ...                   ...   \n",
       "35701              1.0          0.0                   0.0   \n",
       "35702              5.0          1.0                   0.0   \n",
       "35703              NaN          1.0                   0.0   \n",
       "35704              NaN          1.0                   1.0   \n",
       "35705              NaN          1.0                   1.0   \n",
       "\n",
       "       has_health_insurance_gap  \n",
       "0                           0.0  \n",
       "1                           0.0  \n",
       "2                           0.0  \n",
       "3                           1.0  \n",
       "4                           0.0  \n",
       "...                         ...  \n",
       "35701                       NaN  \n",
       "35702                       NaN  \n",
       "35703                       NaN  \n",
       "35704                       1.0  \n",
       "35705                       0.0  \n",
       "\n",
       "[35706 rows x 28 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_cdc_clean = df_cdc_clean[cols_to_keep]\n",
    "\n",
    "df_T = df_cdc_clean.describe().T\n",
    "cols_w_volume = df_T[df_T['count'] >= df_T['count'].max()*0.5].index.tolist()\n",
    "df_cdc_clean = df_cdc_clean [cols_w_volume]\n",
    "\n",
    "df_cdc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProfileReport(df_cdc_clean, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQN and MDD are the first two columns in the df, so exclude from X\n",
    "X = df_cdc_clean.iloc[:,2:].values\n",
    "y = df_cdc_clean['MDD'].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28564, 26)\n",
      "(7142, 26)\n",
      "(28564,)\n",
      "(7142,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(pred_labels, y_test, algo_name, show_full_report=False):\n",
    "    eval_on = 'macro avg' \n",
    "    # eval_on = 'depressed' #not_depressed\n",
    "    target_names = ['not_depressed', 'depressed',]\n",
    "    \n",
    "    df_cls_rpt = pd.DataFrame(\n",
    "        classification_report(\n",
    "            y_test, \n",
    "            pred_labels, \n",
    "            target_names=target_names, \n",
    "            output_dict=True\n",
    "        )\n",
    "    ).rename_axis('metric')\\\n",
    "    .reset_index()\n",
    "\n",
    "    \n",
    "    if show_full_report == True:\n",
    "        display(df_cls_rpt)\n",
    "    accuracy = df_cls_rpt[['accuracy']].iloc[0, 0]\n",
    "    df_cls_rpt = df_cls_rpt[['metric', eval_on]].T\n",
    "    df_cls_rpt.columns = df_cls_rpt.iloc[0,:]\n",
    "    df_cls_rpt = df_cls_rpt.iloc[1:,:]\n",
    "    df_cls_rpt['accuracy'] = accuracy\n",
    "\n",
    "    df_cls_rpt['algo'] = algo_name\n",
    "\n",
    "    first_column = df_cls_rpt.pop('algo')\n",
    "    df_cls_rpt.insert(0, 'algo', first_column)\n",
    "\n",
    "    # display(df_cls_rpt)\n",
    "    return df_cls_rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, pred_labels):\n",
    "    \"\"\"\n",
    "    Function that displays a confusion matrix for provided true and predicted classes\n",
    "    \"\"\"\n",
    "    #print(f'cover type 1 and type 2 total correct {np.sum(np.diag(metrics.confusion_matrix(y_test, pred_labels))[:2])}')\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    disp = disp.plot(include_values=True, cmap='viridis', ax=ax, xticks_rotation='horizontal')    \n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_grid_search_pred(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test,\n",
    "    y_test,\n",
    "    algo,\n",
    "    algo_name,\n",
    "    params,\n",
    "    cv,\n",
    "    verbose,\n",
    "    imputer,\n",
    "    scaler):\n",
    "    score_on = 'recall' #'f1_score'\n",
    "\n",
    "    pipeline_list = [imputer, scaler, algo]\n",
    "    # drop imputer or scaler if none\n",
    "    pipeline_list = [step for step in pipeline_list if step is not None]\n",
    "\n",
    "    processing_pipeline = make_pipeline(imputer, scaler, algo)\n",
    "    processing_pipeline = make_pipeline()\n",
    "    i = 1\n",
    "    for step in pipeline_list:\n",
    "        processing_pipeline.steps.append([(type(step).__name__).lower(), step])\n",
    "        i += 1\n",
    "    grid = GridSearchCV(\n",
    "        processing_pipeline, \n",
    "        param_grid=params, \n",
    "        n_jobs=-1, \n",
    "        cv=cv, \n",
    "        verbose=verbose, \n",
    "        scoring='recall')\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    pred_labels = [x.round() for x in grid.best_estimator_.predict(X_test)]\n",
    "    pred_labels = [x.round() for x in pred_labels]\n",
    "\n",
    "    df_algo_cls_rpt = get_classification_report(pred_labels, y_test, algo_name)\n",
    "    df_algo_cls_rpt['train_r2'] = grid.best_estimator_.score(X_train, y_train)\n",
    "    df_algo_cls_rpt['test_r2'] = grid.best_estimator_.score(X_test, y_test)\n",
    "    df_algo_cls_rpt['best_params'] = str(grid.best_params_)\n",
    "    tn, fp, fn, tp = confusion_matrix(pred_labels, y_test).ravel()\n",
    "    df_algo_cls_rpt['tp'] = tp\n",
    "    df_algo_cls_rpt['fn'] = fn\n",
    "    df_algo_cls_rpt['fp'] = fp\n",
    "    df_algo_cls_rpt['tn'] = tn\n",
    "\n",
    "    \n",
    "    return df_algo_cls_rpt, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_baseline_pred(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test,\n",
    "    y_test,\n",
    "    show_full_report,\n",
    "    algo,\n",
    "    algo_name,\n",
    "    imputer,\n",
    "    scaler):\n",
    "\n",
    "    pipeline_list = [imputer, scaler, algo]\n",
    "    # drop imputer or scaler if none\n",
    "    pipeline_list = [step for step in pipeline_list if step is not None]\n",
    "    \n",
    "    try:\n",
    "        processing_pipeline = make_pipeline()\n",
    "        i = 1\n",
    "        for step in pipeline_list:\n",
    "            processing_pipeline.steps.append([(type(step).__name__).lower(), step])\n",
    "            i += 1\n",
    "\n",
    "        processing_pipeline.fit(X_train, y_train)\n",
    "        pred_labels  = processing_pipeline.predict(X_test)\n",
    "        pred_labels = [x.round() for x in pred_labels]\n",
    "\n",
    "        df_algo_cls_rpt = get_classification_report(pred_labels, y_test, algo_name, show_full_report)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, pred_labels).ravel()\n",
    "        df_algo_cls_rpt['tp'] = tp\n",
    "        df_algo_cls_rpt['fn'] = fn\n",
    "        df_algo_cls_rpt['fp'] = fp\n",
    "        df_algo_cls_rpt['tn'] = tn\n",
    "        \n",
    "        return df_algo_cls_rpt, pred_labels\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        raise Exception (f'{algo_name} might not work with NaN')\n",
    "        \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def baseline_models(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    algo_attempt_list,\n",
    "    do_smote=False,\n",
    "    show_confusion_matrix=False,\n",
    "    show_full_report=False,\n",
    "    grid_search=False,\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    imputer=SimpleImputer(),\n",
    "    scaler=RobustScaler(),\n",
    "    bin_vars=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # do_smote\n",
    "    if do_smote == True:\n",
    "        # have to impute first because smote won't take nulls\n",
    "        my_imputer = SimpleImputer()\n",
    "        X_train = my_imputer.fit_transform(X_train)\n",
    "        X_test = my_imputer.fit_transform(X_test)\n",
    "\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    if bin_vars == True:\n",
    "        est = KBinsDiscretizer(\n",
    "            n_bins=3,\n",
    "            encode='ordinal', \n",
    "            strategy='uniform', \n",
    "            subsample=None\n",
    "        )\n",
    "        my_imputer = SimpleImputer()\n",
    "        X_train = my_imputer.fit_transform(X_train)\n",
    "        X_test = my_imputer.fit_transform(X_test)\n",
    "        \n",
    "        est.fit(X_train)\n",
    "        X_train = est.transform(X_train)\n",
    "\n",
    "        est.fit(X_test)\n",
    "        X_test = est.transform(X_test)\n",
    "\n",
    "    df_cls_rpt = pd.DataFrame()\n",
    "    conf_mtrx_dict = {}\n",
    "\n",
    "    if algo_attempt_list == 'all':\n",
    "        algo_attempt_list = ['knn', 'lm', 'bnb', 'gnb', 'dt', 'rf', 'gb', 'xgb']\n",
    "    \n",
    "    for a in algo_attempt_list:\n",
    "        # SVR\n",
    "        if a == 'svr':\n",
    "            algo_name = 'SVR'\n",
    "            params = {\n",
    "                \"simpleimputer__strategy\": [\"mean\", \"median\"],\n",
    "                \"robustscaler__quantile_range\": [(25.0, 75.0), (30.0, 70.0)],\n",
    "                \"svr__C\": [0.1, 1.0],\n",
    "                \"svr__gamma\": [\"auto\", 0.1],\n",
    "            }\n",
    "            algo = SVR()\n",
    "        elif a == 'knn':\n",
    "            algo_name = 'KNN'\n",
    "            params = {\n",
    "                'kneighborsclassifier__n_neighbors': list(range(1, 15))\n",
    "            }\n",
    "            print(params)\n",
    "            algo = KNeighborsClassifier()\n",
    "        elif a == 'lm':\n",
    "            algo_name = 'Logistic Regression'\n",
    "            params = {\n",
    "                'logisticregression__penalty': ['l1','l2'], \n",
    "                'logisticregression__C': [0.001,0.01,0.1,1,10,100,1000]\n",
    "            }\n",
    "            algo = LogisticRegression(max_iter=1000, penalty='l2', C=10)\n",
    "        elif a == 'bnb':\n",
    "            algo_name = 'Bernoulli Naive Bayes'\n",
    "            params = {\n",
    "                'bernoullinb__alpha': np.logspace(0,-9, num=100),\n",
    "                'bernoullinb__binarize': [0.0, 1.0, 2.0],\n",
    "                # 'bernoullinb__fit_prior': [True, False]\n",
    "            }\n",
    "            algo = BernoulliNB()\n",
    "        elif a == 'gnb':\n",
    "            algo_name = 'Gaussian Naive Bayes'\n",
    "            params = {\n",
    "                'gaussiannb__var_smoothing': np.logspace(0,-9, num=100)\n",
    "            }\n",
    "            algo = GaussianNB()\n",
    "        elif a == 'dt':\n",
    "            algo_name = 'Decision Tree'\n",
    "            params = {\n",
    "                'decisiontreeclassifier__criterion':['gini', 'entropy', 'logloss'],\n",
    "                'decisiontreeclassifier__max_depth': np.arange(1, 15)\n",
    "            }\n",
    "            algo = DecisionTreeClassifier(random_state=42)\n",
    "        elif a == 'rf':\n",
    "            algo_name = 'Random Forest'\n",
    "            params = {\n",
    "                'randomforestclassifier__criterion': ['gini', 'entropy', 'logloss'],\n",
    "                \"randomforestclassifier__n_estimators\": [10, 20, 40, 80, 100, 125, 150],\n",
    "                \"randomforestclassifier__max_features\": [\"sqrt\", \"log2\", None],\n",
    "                \"randomforestclassifier__min_samples_split\": [1, 2, 4, 8],\n",
    "                \"randomforestclassifier__bootstrap\": [True, False],\n",
    "            }\n",
    "            algo = RandomForestClassifier(random_state=42)\n",
    "        elif a == 'gb':\n",
    "            algo_name = 'Gradient Boosting Classifier'\n",
    "            params = {\n",
    "                \"gradientboostingclassifier__loss\":[\"log_loss\", \"exponential\"],\n",
    "                \"gradientboostingclassifier__learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "                # \"gradientboostingclassifier__min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "                # \"gradientboostingclassifier__min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "                \"gradientboostingclassifier__max_depth\":[3,5,8],\n",
    "                \"gradientboostingclassifier__max_features\":[\"log2\", \"sqrt\"],\n",
    "                \"gradientboostingclassifier__criterion\": [\"friedman_mse\", \"mae\"],\n",
    "                # \"gradientboostingclassifier__subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "                \"gradientboostingclassifier__n_estimators\":[10, 25, 50, 100, 125, 150]\n",
    "            }\n",
    "            algo = GradientBoostingClassifier()\n",
    "        elif a == 'xgb':\n",
    "            algo_name = 'XGBoost'\n",
    "            params = {\n",
    "                'xgbclassifier__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],\n",
    "                'xgbclassifier__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],\n",
    "                \"xgbclassifier__gamma\":[0, 0.25, 0.5, 0.75,1],\n",
    "                'xgbclassifier__n_estimators': [100, 500, 1000],\n",
    "            }\n",
    "            algo = xgb.XGBClassifier()\n",
    "        elif a == 'mlp':\n",
    "            # X = [[0., 0.], [1., 1.]]\n",
    "            # y = [0, 1]\n",
    "            algo_name = 'MLP'\n",
    "            algo = MLPClassifier(\n",
    "                max_iter=1000\n",
    "                # solver='lbfgs', \n",
    "                # alpha=1e-5,\n",
    "                # hidden_layer_sizes=(5, 2),\n",
    "                # random_state=42,\n",
    "            )\n",
    "            params = {\n",
    "                'mlpclassifier__solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "                'mlpclassifier__activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                'mlpclassifier__early_stopping' : [True, False]\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise Exception(f'{a} is not a supported algorithm')\n",
    "            # print(f'{a} is not a supported algorithm')\n",
    "\n",
    "        if grid_search == True and params is not None:\n",
    "            df_algo_cls_rpt, pred_labels = algo_grid_search_pred(\n",
    "                X_train, \n",
    "                y_train, \n",
    "                X_test,\n",
    "                y_test,\n",
    "                algo=algo,\n",
    "                algo_name=algo_name,\n",
    "                params=params,\n",
    "                imputer=imputer,\n",
    "                scaler=scaler,\n",
    "                cv=cv,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            if grid_search == True and params is None:\n",
    "                # print(f'Params not defined for {algo_name} GridSearch. Fitting baseline model.')\n",
    "                pass\n",
    "            df_algo_cls_rpt, pred_labels = algo_baseline_pred(\n",
    "                X_train, \n",
    "                y_train, \n",
    "                X_test,\n",
    "                y_test,\n",
    "                show_full_report,\n",
    "                algo=algo,\n",
    "                algo_name=algo_name,\n",
    "                imputer=imputer,\n",
    "                scaler=scaler,\n",
    "            )\n",
    "\n",
    "        df_cls_rpt = pd.concat([\n",
    "                df_algo_cls_rpt, \n",
    "                df_cls_rpt], \n",
    "                ignore_index=True\n",
    "        )\n",
    "        conf_mtrx_dict[algo_name] = pred_labels\n",
    "    \n",
    "\n",
    "    if show_confusion_matrix:\n",
    "        for k,v in conf_mtrx_dict.items():\n",
    "            print(f'{k} Confusion Matrix')\n",
    "            plot_confusion_matrix(y_test, v)\n",
    "\n",
    "    df_cls_rpt.sort_values(by=['f1-score'], ascending=False, inplace=True)\n",
    "    df_cls_rpt = df_cls_rpt.reset_index(drop=True)\n",
    "    return df_cls_rpt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=1;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=1;, score=0.000 total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=1;, score=0.000 total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=2;, score=0.000 total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=2;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=2;, score=0.000 total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=3;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=3;, score=0.000 total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=4;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=3;, score=0.000 total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=4;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=5;, score=0.000 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=4;, score=0.000 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=6;, score=0.036 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=5;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=6;, score=0.070 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=5;, score=0.041 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=7;, score=0.107 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=7;, score=0.061 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=9;, score=0.121 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=8;, score=0.136 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=6;, score=0.073 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=10;, score=0.133 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=7;, score=0.078 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=9;, score=0.167 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=8;, score=0.085 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=11;, score=0.155 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=9;, score=0.141 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=8;, score=0.109 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=11;, score=0.206 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=10;, score=0.165 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=10;, score=0.150 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=11;, score=0.141 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=12;, score=0.160 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=13;, score=0.177 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=1;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=1;, score=0.000 total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=1;, score=0.000 total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=12;, score=0.182 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=2;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=13;, score=0.203 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=14;, score=0.182 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=13;, score=0.184 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=2;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=12;, score=0.203 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=3;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=2;, score=0.000 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=3;, score=0.000 total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=4;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=5;, score=0.000 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=5;, score=0.000 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=14;, score=0.225 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=6;, score=0.036 total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=4;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=gini, decisiontreeclassifier__max_depth=14;, score=0.174 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=4;, score=0.000 total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=5;, score=0.041 total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=6;, score=0.068 total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=6;, score=0.073 total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=3;, score=0.000 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=7;, score=0.061 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=7;, score=0.097 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=8;, score=0.133 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=9;, score=0.140 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=9;, score=0.116 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=10;, score=0.148 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=11;, score=0.131 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=8;, score=0.085 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=11;, score=0.182 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=8;, score=0.107 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=7;, score=0.070 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=9;, score=0.119 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=10;, score=0.150 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=10;, score=0.116 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=11;, score=0.126 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=12;, score=0.143 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=2;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=13;, score=0.128 total time=   0.1s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=12;, score=0.145 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=2;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=2;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=3;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=13;, score=0.194 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=3;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=4;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=12;, score=0.169 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=5;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=3;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=4;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=14;, score=0.141 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=4;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=5;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=5;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=6;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=13;, score=0.133 total time=   0.1s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=7;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=7;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=6;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=6;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=14;, score=0.138 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=7;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=8;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=8;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=9;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=8;, score=nan total time=   0.0s[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=11;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=9;, score=nan total time=   0.0s\n",
      "\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=10;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=9;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=entropy, decisiontreeclassifier__max_depth=14;, score=0.196 total time=   0.1s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=11;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=10;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=10;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=11;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=12;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=12;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=12;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=13;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=13;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=13;, score=nan total time=   0.0s\n",
      "[CV 2/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=14;, score=nan total time=   0.0s\n",
      "[CV 1/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=14;, score=nan total time=   0.0s\n",
      "[CV 3/3] END decisiontreeclassifier__criterion=logloss, decisiontreeclassifier__max_depth=14;, score=nan total time=   0.0s\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n",
      "[CV 2/3] END logisticregression__C=0.001, logisticregression__penalty=l1;, score=nan total time=   0.0s[CV 1/3] END logisticregression__C=0.001, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "\n",
      "[CV 3/3] END logisticregression__C=0.001, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=0.01, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=0.001, logisticregression__penalty=l2;, score=0.027 total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=0.001, logisticregression__penalty=l2;, score=0.034 total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=0.001, logisticregression__penalty=l2;, score=0.029 total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=0.01, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=0.01, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=0.1, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=0.1, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=0.1, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=0.01, logisticregression__penalty=l2;, score=0.075 total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=0.01, logisticregression__penalty=l2;, score=0.087 total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=1, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=0.01, logisticregression__penalty=l2;, score=0.082 total time=   0.1s\n",
      "[CV 3/3] END logisticregression__C=1, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=10, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=0.1, logisticregression__penalty=l2;, score=0.090 total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=1, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=10, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=10, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=0.1, logisticregression__penalty=l2;, score=0.097 total time=   0.1s\n",
      "[CV 2/3] END logisticregression__C=1, logisticregression__penalty=l2;, score=0.090 total time=   0.1s\n",
      "[CV 1/3] END logisticregression__C=100, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=100, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=100, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=0.1, logisticregression__penalty=l2;, score=0.099 total time=   0.1s\n",
      "[CV 1/3] END logisticregression__C=1, logisticregression__penalty=l2;, score=0.097 total time=   0.1s\n",
      "[CV 1/3] END logisticregression__C=1000, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END logisticregression__C=10, logisticregression__penalty=l2;, score=0.090 total time=   0.1s\n",
      "[CV 1/3] END logisticregression__C=10, logisticregression__penalty=l2;, score=0.097 total time=   0.1s\n",
      "[CV 3/3] END logisticregression__C=1, logisticregression__penalty=l2;, score=0.104 total time=   0.1s\n",
      "[CV 2/3] END logisticregression__C=1000, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=1000, logisticregression__penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END logisticregression__C=100, logisticregression__penalty=l2;, score=0.097 total time=   0.1s\n",
      "[CV 2/3] END logisticregression__C=100, logisticregression__penalty=l2;, score=0.090 total time=   0.1s\n",
      "[CV 3/3] END logisticregression__C=100, logisticregression__penalty=l2;, score=0.104 total time=   0.1s\n",
      "[CV 1/3] END logisticregression__C=1000, logisticregression__penalty=l2;, score=0.097 total time=   0.1s\n",
      "[CV 2/3] END logisticregression__C=1000, logisticregression__penalty=l2;, score=0.090 total time=   0.0s\n",
      "[CV 3/3] END logisticregression__C=10, logisticregression__penalty=l2;, score=0.104 total time=   0.1s\n",
      "[CV 3/3] END logisticregression__C=1000, logisticregression__penalty=l2;, score=0.104 total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>metric</th>\n",
       "      <th>algo</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>best_params</th>\n",
       "      <th>tp</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.687646</td>\n",
       "      <td>0.539568</td>\n",
       "      <td>0.559521</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.960235</td>\n",
       "      <td>0.956869</td>\n",
       "      <td>0.960235</td>\n",
       "      <td>{'logisticregression__C': 1, 'logisticregressi...</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>251</td>\n",
       "      <td>6835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.558466</td>\n",
       "      <td>0.5496</td>\n",
       "      <td>0.553522</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.939093</td>\n",
       "      <td>0.984911</td>\n",
       "      <td>0.939093</td>\n",
       "      <td>{'decisiontreeclassifier__criterion': 'gini', ...</td>\n",
       "      <td>35</td>\n",
       "      <td>196</td>\n",
       "      <td>239</td>\n",
       "      <td>6672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric                 algo precision    recall  f1-score support  accuracy  \\\n",
       "0       Logistic Regression  0.687646  0.539568  0.559521  7142.0  0.960235   \n",
       "1             Decision Tree  0.558466    0.5496  0.553522  7142.0  0.939093   \n",
       "\n",
       "metric  train_r2   test_r2                                        best_params  \\\n",
       "0       0.956869  0.960235  {'logisticregression__C': 1, 'logisticregressi...   \n",
       "1       0.984911  0.939093  {'decisiontreeclassifier__criterion': 'gini', ...   \n",
       "\n",
       "metric  tp   fn   fp    tn  \n",
       "0       23   33  251  6835  \n",
       "1       35  196  239  6672  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "baseline_models(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    algo_attempt_list=['dt', 'lm'],\n",
    "    grid_search=True,\n",
    "    cv=3,\n",
    "    verbose=3,\n",
    "    imputer=SimpleImputer(),\n",
    "    scaler=StandardScaler(),\n",
    "    bin_vars=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force \n",
    "Can select different algo, imputer, scaler, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ##### with all setting each trial takes a while ##########\n",
    "# ##### commenting out options can make bring iteration time down drastically ###########\n",
    "# num_trials = 2\n",
    "# best_score = 0\n",
    "# primary_eval_metric = 'recall'\n",
    "# secondary_eval_metric = 'f1-score'\n",
    "# best_cols = []\n",
    "# df_best_scores = pd.DataFrame()\n",
    "\n",
    "# try:\n",
    "    \n",
    "#     for i in tqdm(range(1, num_trials+1)):\n",
    "#         df_random_feats = df_cdc_clean.iloc[:,2:]\n",
    "#         rand_num_features = random.randint(3, (df_random_feats.shape[1]))\n",
    "#         df_random_feats = df_random_feats.sample(n=rand_num_features, axis='columns')\n",
    "#         X = df_random_feats\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(\n",
    "#             X, y, test_size=0.2, random_state=42\n",
    "#         )\n",
    "\n",
    "#         # SMOTE or not\n",
    "#         for b in [True, False]:\n",
    "#             # different imputers\n",
    "#             for imp in [KNNImputer(), SimpleImputer(strategy='mean'), SimpleImputer(strategy='most_frequent')]:\n",
    "#                 for scl in [RobustScaler(), StandardScaler(), MinMaxScaler(), Normalizer()]:\n",
    "#                     df_scores = baseline_models(\n",
    "#                         X_train, \n",
    "#                         y_train, \n",
    "#                         X_test, \n",
    "#                         y_test, \n",
    "#                         do_smote=b, \n",
    "#                         algo_attempt_list=['bnb', 'gnb', 'lm'],\n",
    "#                         grid_search=True,\n",
    "#                         cv=5,\n",
    "#                         verbose=0,\n",
    "#                         imputer=imp,\n",
    "#                         scaler=scl\n",
    "#                     )\n",
    "#                     df_scores['SMOTE'] = b\n",
    "#                     df_scores['imputer'] = imp\n",
    "#                     df_scores['scaler'] = scl\n",
    "#                     # sort to keep best\n",
    "#                     df_scores.sort_values(\n",
    "#                         by=[primary_eval_metric, secondary_eval_metric], \n",
    "#                         inplace=True, \n",
    "#                         ascending=False\n",
    "#                     )\n",
    "#                     df_scores = df_scores.reset_index()\n",
    "#                     df_scores = df_scores.iloc[:,1:]\n",
    "#                     if df_scores[primary_eval_metric][0] > best_score:\n",
    "#                         best_f1 = df_scores[primary_eval_metric][0]\n",
    "#                         best_cols = df_random_feats.columns\n",
    "#                         df_scores['features'] = str(df_random_feats.columns.tolist())\n",
    "#                         df_scores['trial_num'] = i\n",
    "#                         df_best_scores = pd.concat([df_best_scores, df_scores.iloc[0:1,:]], ignore_index=True)\n",
    "# except KeyboardInterrupt as e:\n",
    "#     print(e)\n",
    "\n",
    "# df_best_scores.sort_values(\n",
    "#     by=[primary_eval_metric, secondary_eval_metric], \n",
    "#     inplace=True, \n",
    "#     ascending=False\n",
    "# )\n",
    "# df_best_scores = df_best_scores.reset_index(drop=True)\n",
    "# print(best_f1)\n",
    "# print(best_cols)\n",
    "# df_best_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized choices for imputer, SMOTE, and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [26:53<4:31:52, 179.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5307794350293419\n",
      "Index(['general_health_condition', 'type_of_work_done_last_week',\n",
      "       'poor_appetitie_or_overeating', 'food_security_level_household',\n",
      "       'moving_or_speaking_to_slowly_or_fast', 'has_health_insurance_gap',\n",
      "       'feeling_down_depressed_hopeless', 'feeling_bad_about_yourself',\n",
      "       'education_level', 'trouble_concentrating',\n",
      "       'monthly_poverty_index_category', 'has_overweight_diagnosis',\n",
      "       'has_tried_to_lose_weight_12mo', 'age_in_years',\n",
      "       'thoughts_you_would_be_better_off_dead', 'has_diabetes',\n",
      "       'annual_healthcare_visit_count', 'little_interest_in_doing_things',\n",
      "       'monthly_poverty_index', 'is_usa_born', 'has_smoked_tabacco_last_5days',\n",
      "       'how_healthy_is_your_diet'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>metric</th>\n",
       "      <th>algo</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>best_params</th>\n",
       "      <th>tp</th>\n",
       "      <th>fn</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>SMOTE</th>\n",
       "      <th>bin_vars</th>\n",
       "      <th>imputer</th>\n",
       "      <th>scaler</th>\n",
       "      <th>features</th>\n",
       "      <th>trial_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.574643</td>\n",
       "      <td>0.551051</td>\n",
       "      <td>0.559933</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.945253</td>\n",
       "      <td>0.980465</td>\n",
       "      <td>0.945253</td>\n",
       "      <td>{'decisiontreeclassifier__criterion': 'gini', ...</td>\n",
       "      <td>34</td>\n",
       "      <td>151</td>\n",
       "      <td>240</td>\n",
       "      <td>6717</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>RobustScaler()</td>\n",
       "      <td>['is_usa_born', 'has_overweight_diagnosis', 'a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.563422</td>\n",
       "      <td>0.554928</td>\n",
       "      <td>0.558755</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.939233</td>\n",
       "      <td>0.983931</td>\n",
       "      <td>0.939233</td>\n",
       "      <td>{'decisiontreeclassifier__criterion': 'gini', ...</td>\n",
       "      <td>38</td>\n",
       "      <td>198</td>\n",
       "      <td>236</td>\n",
       "      <td>6670</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>['monthly_poverty_index_category', 'little_int...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.610339</td>\n",
       "      <td>0.530667</td>\n",
       "      <td>0.543475</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.956595</td>\n",
       "      <td>0.965341</td>\n",
       "      <td>0.956595</td>\n",
       "      <td>{'decisiontreeclassifier__criterion': 'gini', ...</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "      <td>255</td>\n",
       "      <td>6813</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>['thoughts_you_would_be_better_off_dead', 'has...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.542334</td>\n",
       "      <td>0.670952</td>\n",
       "      <td>0.537941</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.818678</td>\n",
       "      <td>0.916307</td>\n",
       "      <td>0.818678</td>\n",
       "      <td>{'decisiontreeclassifier__criterion': 'gini', ...</td>\n",
       "      <td>140</td>\n",
       "      <td>1161</td>\n",
       "      <td>134</td>\n",
       "      <td>5707</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>['has_health_insurance', 'general_health_condi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.553422</td>\n",
       "      <td>0.774174</td>\n",
       "      <td>0.534627</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.767852</td>\n",
       "      <td>0.765901</td>\n",
       "      <td>0.767852</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>214</td>\n",
       "      <td>1598</td>\n",
       "      <td>60</td>\n",
       "      <td>5270</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>['feeling_tired_or_having_little_energy', 'ann...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.544478</td>\n",
       "      <td>0.703327</td>\n",
       "      <td>0.532661</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.793335</td>\n",
       "      <td>0.897204</td>\n",
       "      <td>0.793335</td>\n",
       "      <td>{'decisiontreeclassifier__criterion': 'gini', ...</td>\n",
       "      <td>166</td>\n",
       "      <td>1368</td>\n",
       "      <td>108</td>\n",
       "      <td>5500</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SimpleImputer(strategy='most_frequent')</td>\n",
       "      <td>Normalizer()</td>\n",
       "      <td>['has_health_insurance_gap', 'food_security_le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.538975</td>\n",
       "      <td>0.662862</td>\n",
       "      <td>0.530779</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.809857</td>\n",
       "      <td>0.901120</td>\n",
       "      <td>0.809857</td>\n",
       "      <td>{'decisiontreeclassifier__criterion': 'gini', ...</td>\n",
       "      <td>138</td>\n",
       "      <td>1222</td>\n",
       "      <td>136</td>\n",
       "      <td>5646</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>SimpleImputer()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>['general_health_condition', 'type_of_work_don...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.548656</td>\n",
       "      <td>0.755868</td>\n",
       "      <td>0.524286</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.756231</td>\n",
       "      <td>0.754940</td>\n",
       "      <td>0.756231</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>207</td>\n",
       "      <td>1674</td>\n",
       "      <td>67</td>\n",
       "      <td>5194</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>StandardScaler()</td>\n",
       "      <td>['type_of_work_done_last_week', 'general_healt...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.547605</td>\n",
       "      <td>0.757343</td>\n",
       "      <td>0.518813</td>\n",
       "      <td>7142.0</td>\n",
       "      <td>0.745589</td>\n",
       "      <td>0.743138</td>\n",
       "      <td>0.745589</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>211</td>\n",
       "      <td>1754</td>\n",
       "      <td>63</td>\n",
       "      <td>5114</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>KNNImputer()</td>\n",
       "      <td>MinMaxScaler()</td>\n",
       "      <td>['feeling_down_depressed_hopeless', 'trouble_c...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric                 algo precision    recall  f1-score support  accuracy  \\\n",
       "0             Decision Tree  0.574643  0.551051  0.559933  7142.0  0.945253   \n",
       "1             Decision Tree  0.563422  0.554928  0.558755  7142.0  0.939233   \n",
       "2             Decision Tree  0.610339  0.530667  0.543475  7142.0  0.956595   \n",
       "3             Decision Tree  0.542334  0.670952  0.537941  7142.0  0.818678   \n",
       "4       Logistic Regression  0.553422  0.774174  0.534627  7142.0  0.767852   \n",
       "5             Decision Tree  0.544478  0.703327  0.532661  7142.0  0.793335   \n",
       "6             Decision Tree  0.538975  0.662862  0.530779  7142.0  0.809857   \n",
       "7       Logistic Regression  0.548656  0.755868  0.524286  7142.0  0.756231   \n",
       "8       Logistic Regression  0.547605  0.757343  0.518813  7142.0  0.745589   \n",
       "\n",
       "metric  train_r2   test_r2                                        best_params  \\\n",
       "0       0.980465  0.945253  {'decisiontreeclassifier__criterion': 'gini', ...   \n",
       "1       0.983931  0.939233  {'decisiontreeclassifier__criterion': 'gini', ...   \n",
       "2       0.965341  0.956595  {'decisiontreeclassifier__criterion': 'gini', ...   \n",
       "3       0.916307  0.818678  {'decisiontreeclassifier__criterion': 'gini', ...   \n",
       "4       0.765901  0.767852  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "5       0.897204  0.793335  {'decisiontreeclassifier__criterion': 'gini', ...   \n",
       "6       0.901120  0.809857  {'decisiontreeclassifier__criterion': 'gini', ...   \n",
       "7       0.754940  0.756231  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "8       0.743138  0.745589  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "\n",
       "metric   tp    fn   fp    tn  SMOTE  bin_vars  \\\n",
       "0        34   151  240  6717  False      True   \n",
       "1        38   198  236  6670  False      True   \n",
       "2        19    55  255  6813  False     False   \n",
       "3       140  1161  134  5707   True      True   \n",
       "4       214  1598   60  5270   True      True   \n",
       "5       166  1368  108  5500   True      True   \n",
       "6       138  1222  136  5646   True      True   \n",
       "7       207  1674   67  5194   True     False   \n",
       "8       211  1754   63  5114   True      True   \n",
       "\n",
       "metric                                  imputer            scaler  \\\n",
       "0                                  KNNImputer()    RobustScaler()   \n",
       "1                               SimpleImputer()    MinMaxScaler()   \n",
       "2       SimpleImputer(strategy='most_frequent')      Normalizer()   \n",
       "3       SimpleImputer(strategy='most_frequent')      Normalizer()   \n",
       "4                               SimpleImputer()  StandardScaler()   \n",
       "5       SimpleImputer(strategy='most_frequent')      Normalizer()   \n",
       "6                               SimpleImputer()    MinMaxScaler()   \n",
       "7                                  KNNImputer()  StandardScaler()   \n",
       "8                                  KNNImputer()    MinMaxScaler()   \n",
       "\n",
       "metric                                           features  trial_num  \n",
       "0       ['is_usa_born', 'has_overweight_diagnosis', 'a...          4  \n",
       "1       ['monthly_poverty_index_category', 'little_int...          7  \n",
       "2       ['thoughts_you_would_be_better_off_dead', 'has...          5  \n",
       "3       ['has_health_insurance', 'general_health_condi...          8  \n",
       "4       ['feeling_tired_or_having_little_energy', 'ann...          2  \n",
       "5       ['has_health_insurance_gap', 'food_security_le...          1  \n",
       "6       ['general_health_condition', 'type_of_work_don...          9  \n",
       "7       ['type_of_work_done_last_week', 'general_healt...          3  \n",
       "8       ['feeling_down_depressed_hopeless', 'trouble_c...          6  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "##### with all setting each trial takes a while ##########\n",
    "##### commenting out options can make bring iteration time down drastically ###########\n",
    "num_trials = 100\n",
    "best_score = 0\n",
    "primary_eval_metric = 'f1-score'\n",
    "secondary_eval_metric = 'recall' \n",
    "\n",
    "best_cols = []\n",
    "df_best_scores = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    \n",
    "    for i in tqdm(range(1, num_trials+1)):\n",
    "        df_random_feats = df_cdc_clean.iloc[:,2:]\n",
    "        rand_num_features = random.randint(10, (df_random_feats.shape[1]))\n",
    "        df_random_feats = df_random_feats.sample(n=rand_num_features, axis='columns')\n",
    "        X = df_random_feats\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # SMOTE or not\n",
    "        for b in [random.choice([True, False])]:\n",
    "            for bv in [random.choice([True, False])]:\n",
    "                # different imputers\n",
    "                for imp in [random.choice([KNNImputer(), SimpleImputer(strategy='mean'), SimpleImputer(strategy='most_frequent')])]:\n",
    "                    for scl in [random.choice([RobustScaler(), StandardScaler(), MinMaxScaler(), Normalizer()])]:\n",
    "                        df_scores = baseline_models(\n",
    "                            X_train, \n",
    "                            y_train, \n",
    "                            X_test, \n",
    "                            y_test, \n",
    "                            do_smote=b, \n",
    "                            algo_attempt_list=['dt', 'lm'],\n",
    "                            grid_search=True,\n",
    "                            cv=5,\n",
    "                            verbose=0,\n",
    "                            imputer=imp,\n",
    "                            scaler=scl,\n",
    "                            bin_vars=bv\n",
    "                        )\n",
    "                        df_scores['SMOTE'] = b\n",
    "                        df_scores['bin_vars'] = bv\n",
    "                        df_scores['imputer'] = imp\n",
    "                        df_scores['scaler'] = scl\n",
    "                        # sort to keep best\n",
    "                        df_scores.sort_values(\n",
    "                            by=[primary_eval_metric, secondary_eval_metric], \n",
    "                            inplace=True, \n",
    "                            ascending=False\n",
    "                        )\n",
    "                        df_scores = df_scores.reset_index()\n",
    "                        df_scores = df_scores.iloc[:,1:]\n",
    "                        if df_scores[primary_eval_metric][0] > best_score:\n",
    "                            best_f1 = df_scores[primary_eval_metric][0]\n",
    "                            best_cols = df_random_feats.columns\n",
    "                            df_scores['features'] = str(df_random_feats.columns.tolist())\n",
    "                            df_scores['trial_num'] = i\n",
    "                            df_best_scores = pd.concat([df_best_scores, df_scores.iloc[0:1,:]], ignore_index=True)\n",
    "except KeyboardInterrupt as e:\n",
    "    print(e)\n",
    "\n",
    "df_best_scores.sort_values(\n",
    "    by=[primary_eval_metric, secondary_eval_metric], \n",
    "    inplace=True, \n",
    "    ascending=False\n",
    ")\n",
    "df_best_scores = df_best_scores.reset_index(drop=True)\n",
    "print(best_f1)\n",
    "print(best_cols)\n",
    "df_best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_best_scores.iloc[0,:]['features'])\n",
    "X = df_cdc_clean[ast.literal_eval(df_best_scores.iloc[0,:]['features'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "baseline_models(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    do_smote=True,\n",
    "    show_confusion_matrix=True,\n",
    "    algo_attempt_list=['lm'],\n",
    "    show_full_report=True,\n",
    "    scaler=MinMaxScaler()\t\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With subset of selected features\n",
    "\n",
    "Logistic Regression model that performed decent. Using this for the trainer.py for API standup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subset = [\n",
    "    'seen_mental_health_professional',\n",
    "    'times_with_12plus_alc',\n",
    "    'time_since_last_healthcare',\n",
    "    'duration_last_healthcare_visit',\n",
    "    'horomones_not_bc',\n",
    "    'count_days_seen_doctor_12mo',\n",
    "    'count_lost_10plus_pounds',\n",
    "    'respiratory_issues',\n",
    "    'arthritis',\n",
    "    'age_in_years',\n",
    "    'is_usa_born',\n",
    "    'regular_periods',\n",
    "    'high_bp',\n",
    "    'metal_objects',\n",
    "    'count_minutes_moderate_sedentary_activity',\n",
    "    'cholesterol_prescription',\n",
    "    'chest_discomfort',\n",
    "    'emergency_food_received'\n",
    "\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_name = 'Logistic Regression'\n",
    "print(df_best_scores.iloc[0,:]['features'])\n",
    "X = df_cdc_clean[feature_subset]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "X_train = my_imputer.fit_transform(X_train)\n",
    "X_test = my_imputer.fit_transform(X_test)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "processing_pipeline = make_pipeline(SimpleImputer(), MinMaxScaler(), LogisticRegression(max_iter=1000, penalty='l2', C=10))\n",
    "\n",
    "\n",
    "processing_pipeline.fit(X_train, y_train)\n",
    "pred_labels  = processing_pipeline.predict(X_test)\n",
    "pred_labels = [x.round() for x in pred_labels]\n",
    "\n",
    "df_algo_cls_rpt = get_classification_report(pred_labels, y_test, algo_name, show_full_report=True)\n",
    "tn, fp, fn, tp = confusion_matrix(pred_labels, y_test).ravel()\n",
    "df_algo_cls_rpt['tp'] = tp\n",
    "df_algo_cls_rpt['fn'] = fn\n",
    "df_algo_cls_rpt['fp'] = fp\n",
    "df_algo_cls_rpt['tn'] = tn\n",
    "df_algo_cls_rpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
