{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orencarmeli/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# TODO: add annotations describing usage of different modules\n",
    "\n",
    "from operator import mod\n",
    "from os import getcwd\n",
    "from os.path import exists, join\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler, KBinsDiscretizer\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from ydata_profiling import ProfileReport\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "# import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import pickle\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "import altair as alt\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set seaborn whitegrid theme\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from random import sample\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_data(original_df, \n",
    "                   columns, \n",
    "                   test_size_to_use=0.2,\n",
    "                   drop_null_rows=False,\n",
    "                   null_imputer_strategy='median', # mean, median, most_frequent\n",
    "                   use_value_scaler=True,\n",
    "                   use_smote=False,\n",
    "                   return_indices=False):\n",
    "    \"\"\"\n",
    "    Function to build feature & indicator matrices for both train & test.\n",
    "    \"\"\"\n",
    "    \n",
    "    # add target column (MDD)\n",
    "    cols_to_use = columns.copy()\n",
    "    cols_to_use.insert(0, 'MDD')\n",
    "    #cols_to_use.insert(0, 'SEQN')\n",
    "    \n",
    "    df_to_use = original_df[cols_to_use]\n",
    "    \n",
    "    if drop_null_rows:\n",
    "        df_to_use.dropna(inplace=True)\n",
    "    \n",
    "    # Create test & train data\n",
    "    x = df_to_use.iloc[:,1:].values\n",
    "    y = df_to_use['MDD'].values\n",
    "    indices = np.arange(y.shape[0])\n",
    "    \n",
    "    if not drop_null_rows:\n",
    "        # SimpleImputer() = fill in missing values\n",
    "        # note imputer may drop columns if no values exist for it\n",
    "        imputer = SimpleImputer(strategy=null_imputer_strategy)  \n",
    "        x = imputer.fit_transform(x)\n",
    "\n",
    "    # RobustScaler() = scale features to remove outliers\n",
    "    if use_value_scaler:\n",
    "        trans = RobustScaler()\n",
    "        x = trans.fit_transform(x)\n",
    "\n",
    "    x_train, x_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        x, \n",
    "        y, \n",
    "        indices,\n",
    "        test_size=test_size_to_use, \n",
    "        random_state=42\n",
    "    ) \n",
    "    \n",
    "    # Technique to de-risk from positive class imbalance\n",
    "    if use_smote:\n",
    "        sm = SMOTE(random_state=42)\n",
    "        x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "    \n",
    "    if return_indices:\n",
    "        return x_train, x_test, y_train, y_test, idx_train, idx_test\n",
    "    else:\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "def get_performance_df(label_actual, label_pred, model_name):\n",
    "    \"\"\"\n",
    "    Function to calculate performance metrics for model.\n",
    "    Includes precision, recal, F1, & support.\n",
    "    \"\"\"\n",
    "    # create classification report\n",
    "    result_table = classification_report(label_actual, label_pred, output_dict=True)\n",
    "    result_table = pd.DataFrame.from_dict(result_table)\n",
    "\n",
    "    # store for later\n",
    "    accuracies = result_table['accuracy'][0]\n",
    "    \n",
    "    column_key = {\n",
    "        '0':'Depressed (No)',\n",
    "         '1':'Depressed (Yes)',\n",
    "         'accuracy':'accuracy',\n",
    "         'macro avg':'Macro Avg',\n",
    "         'weighted avg':'Weighted Avg'\n",
    "    }\n",
    "\n",
    "    # rename grouping\n",
    "    result_table.columns = [column_key.get(key) for key in result_table.columns]\n",
    "\n",
    "    # create dataframe with 1 row per grouping\n",
    "    result_table.drop(labels = 'accuracy', axis = 1, inplace=True)\n",
    "    result_table = result_table.transpose()\n",
    "    result_table['accuracy'] = [accuracies for i in range(result_table.shape[0])]\n",
    "    result_table = result_table.reset_index()\n",
    "    result_table.rename(columns = {'index':'grouping'},inplace=True)\n",
    "    result_table['model'] = model_name\n",
    "    result_table = result_table[['model','grouping','precision','recall','f1-score','support','accuracy']]\n",
    "    return result_table    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35706, 863)\n",
      "(7741, 864)\n"
     ]
    }
   ],
   "source": [
    "cdc_survey = pd.read_csv('../../../data/cdc_nhanes_survey_responses_clean.csv')\n",
    "print(cdc_survey.shape)\n",
    "\n",
    "# filter to pregnant moms\n",
    "cdc_survey_pmom = cdc_survey[cdc_survey['has_been_pregnant'] == 1].reset_index()\n",
    "print(cdc_survey_pmom.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3347, 865)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_screener_cols = [\n",
    "    'little_interest_in_doing_things',\n",
    "    'feeling_down_depressed_hopeless',\n",
    "    'trouble_falling_or_staying_asleep',\n",
    "    'feeling_tired_or_having_little_energy',\n",
    "    'poor_appetitie_or_overeating',\n",
    "    'feeling_bad_about_yourself',\n",
    "    'trouble_concentrating',\n",
    "    'moving_or_speaking_to_slowly_or_fast',\n",
    "    'thoughts_you_would_be_better_off_dead',\n",
    "    'difficult_doing_daytoday_tasks'\n",
    "]\n",
    "\n",
    "cdc_survey_pmom['num_dep_screener_0'] = (cdc_survey_pmom[dep_screener_cols]==0).sum(axis=1)\n",
    "cdc_survey_pmom = cdc_survey_pmom[cdc_survey_pmom['num_dep_screener_0'] >= 9]\n",
    "cdc_survey_pmom.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBC Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_features = [\n",
    "    'times_with_12plus_alc',\n",
    "    'seen_mental_health_professional',\n",
    "    'count_days_seen_doctor_12mo',\n",
    "    'count_lost_10plus_pounds',\n",
    "    'arthritis',\n",
    "    'horomones_not_bc',\n",
    "    'is_usa_born',\n",
    "    'times_with_8plus_alc',\n",
    "    'time_since_last_healthcare',\n",
    "    'duration_last_healthcare_visit',\n",
    "    'work_schedule',\n",
    "    'age_in_years'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2677, 12)\n",
      "(670, 12)\n",
      "(2677,)\n",
      "(670,)\n",
      "(849, 12)\n",
      "(849,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>grouping</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>Depressed (No)</td>\n",
       "      <td>0.967239</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.967994</td>\n",
       "      <td>640.0</td>\n",
       "      <td>0.938806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>Depressed (Yes)</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.938806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>Macro Avg</td>\n",
       "      <td>0.638792</td>\n",
       "      <td>0.634375</td>\n",
       "      <td>0.636539</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.938806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>Weighted Avg</td>\n",
       "      <td>0.937826</td>\n",
       "      <td>0.938806</td>\n",
       "      <td>0.938311</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.938806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model         grouping  precision    recall  \\\n",
       "0  Gradient Boosting Classifier   Depressed (No)   0.967239  0.968750   \n",
       "1  Gradient Boosting Classifier  Depressed (Yes)   0.310345  0.300000   \n",
       "2  Gradient Boosting Classifier        Macro Avg   0.638792  0.634375   \n",
       "3  Gradient Boosting Classifier     Weighted Avg   0.937826  0.938806   \n",
       "\n",
       "   f1-score  support  accuracy  \n",
       "0  0.967994    640.0  0.938806  \n",
       "1  0.305085     30.0  0.938806  \n",
       "2  0.636539    670.0  0.938806  \n",
       "3  0.938311    670.0  0.938806  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_x_train, gb_x_test, gb_y_train, gb_y_test = get_model_data(\n",
    "    original_df = cdc_survey_pmom,\n",
    "    columns = gb_model_features\n",
    ")\n",
    "\n",
    "rus = RandomUnderSampler(\n",
    "    random_state=42, \n",
    "    sampling_strategy=0.12,\n",
    "    replacement=False\n",
    ")\n",
    "gb_x_train_rus, gb_y_train_rus = rus.fit_resample(gb_x_train,gb_y_train)\n",
    "\n",
    "print(gb_x_train.shape)\n",
    "print(gb_x_test.shape)\n",
    "print(gb_y_train.shape)\n",
    "print(gb_y_test.shape)\n",
    "print(gb_x_train_rus.shape)\n",
    "print(gb_y_train_rus.shape)\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(gb_x_train_rus, gb_y_train_rus)\n",
    "gb_pred = gb.predict(gb_x_test)\n",
    "gb_score = get_performance_df(gb_y_test, gb_pred,'Gradient Boosting Classifier')\n",
    "gb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': ['log_loss', 'exponential', 'deviance'],\n",
       " 'learning_rate': [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
       " 'min_samples_split': array([0.1       , 0.13636364, 0.17272727, 0.20909091, 0.24545455,\n",
       "        0.28181818, 0.31818182, 0.35454545, 0.39090909, 0.42727273,\n",
       "        0.46363636, 0.5       ]),\n",
       " 'min_samples_leaf': array([0.1       , 0.13636364, 0.17272727, 0.20909091, 0.24545455,\n",
       "        0.28181818, 0.31818182, 0.35454545, 0.39090909, 0.42727273,\n",
       "        0.46363636, 0.5       ]),\n",
       " 'max_depth': [2, 3, 5, 8],\n",
       " 'max_features': ['log2', 'sqrt'],\n",
       " 'criterion': ['friedman_mse', 'mae', 'squared_error'],\n",
       " 'subsample': [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
       " 'n_estimators': [10, 30, 70, 100, 200]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/hatone/gradientboostingclassifier-with-gridsearchcv/script\n",
    "\n",
    "gb_param_grid = {\n",
    "    \"loss\":[\"log_loss\",\"exponential\",\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[2,3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\", \"mae\",\"squared_error\"],\n",
    "    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10, 30, 70, 100, 200]\n",
    "}\n",
    "#gb_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_grid_search = GridSearchCV(\n",
    "    GradientBoostingClassifier(),\n",
    "    param_grid=gb_param_grid\n",
    ")\n",
    "gb_grid_search.fit(gb_x_train_rus, gb_y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gb_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gb_x_train_rus.shape)\n",
    "\n",
    "gb_hyper = GradientBoostingClassifier(\n",
    "    n_estimators=,\n",
    "    max_features=,\n",
    "    max_depth=,\n",
    "    min_samples_split=,\n",
    "    min_samples_leaf=,\n",
    "    bootstrap=,\n",
    "    random_state=42\n",
    ")\n",
    "gb_hyper.fit(gb_x_train_rus, gb_y_train_rus)\n",
    "gb_hyper_pred = gb_hyper.predict(gb_x_test)\n",
    "gb_hyper_score = get_performance_df(gb_y_test, gb_hyper_pred,'Gradient Boosting Classifier (Hyper)')\n",
    "gb_hyper_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bin_lookup(\n",
    "        feature,\n",
    "        n_bins,\n",
    "        encode,\n",
    "        strategy,\n",
    "        df_to_use):\n",
    "        # make a new column with _bin suffix\n",
    "        new_column_name = feature + '_bin'\n",
    "\n",
    "        # get non-null values per column\n",
    "        feature_values = df_to_use[feature].dropna()\n",
    "\n",
    "        # reshape to be 1 column\n",
    "        feature_values = feature_values.to_numpy().reshape([feature_values.shape[0],1])\n",
    "\n",
    "        # create bins using estimator\n",
    "        est = KBinsDiscretizer(\n",
    "            n_bins=n_bins,\n",
    "            encode=encode, \n",
    "            strategy=strategy, \n",
    "            subsample=None\n",
    "        )\n",
    "        est.fit(feature_values)\n",
    "        feature_values_bin = pd.DataFrame(est.transform(feature_values))\n",
    "\n",
    "        # dataframe with binned values\n",
    "        feature_values_bin.columns = [new_column_name]\n",
    "\n",
    "        # get original\n",
    "        feature_values = pd.DataFrame(feature_values)\n",
    "        feature_values.columns = ['original']\n",
    "\n",
    "        # merge bin & non-binned values together to make a lookup\n",
    "        feature_values = feature_values.merge(feature_values_bin, left_index=True, right_index=True)\n",
    "        feature_value_bin_lookup = feature_values.groupby(['original',new_column_name]).count().reset_index()\n",
    "\n",
    "        return feature_value_bin_lookup, new_column_name\n",
    "    \n",
    "feature_value_bin_lookup, new_column_name = create_bin_lookup(\n",
    "    'count_days_seen_doctor_12mo',\n",
    "    n_bins=10,\n",
    "    encode='ordinal',\n",
    "    strategy='uniform',\n",
    "    df_to_use=cdc_survey_pmom\n",
    ")  \n",
    "\n",
    "# prevent creating a column if already exists\n",
    "# happens if you run this block multiple times\n",
    "if new_column_name in cdc_survey_pmom.columns:\n",
    "    cdc_survey_pmom.drop(columns=new_column_name,inplace=True)\n",
    "\n",
    "# add bin column in a way that doesn't drop nulls\n",
    "cdc_survey_pmom = cdc_survey_pmom.merge(\n",
    "    feature_value_bin_lookup, \n",
    "    left_on='count_days_seen_doctor_12mo', \n",
    "    right_on='original', \n",
    "    how = 'left'\n",
    ")\n",
    "\n",
    "# drop column called \"original\" as was only used to join\n",
    "cdc_survey_pmom.drop(columns=['original'], inplace=True)  \n",
    "\n",
    "\n",
    "rf_model_features = [\n",
    "    'count_days_seen_doctor_12mo_bin',\n",
    "    'times_with_12plus_alc',\n",
    "    'seen_mental_health_professional',\n",
    "    'count_lost_10plus_pounds',\n",
    "    'arthritis',\n",
    "    'horomones_not_bc',\n",
    "    'is_usa_born',\n",
    "    'times_with_8plus_alc',\n",
    "    'time_since_last_healthcare',\n",
    "    'duration_last_healthcare_visit',\n",
    "    'work_schedule'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2677, 11)\n",
      "(670, 11)\n",
      "(2677,)\n",
      "(670,)\n",
      "(849, 11)\n",
      "(849,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>grouping</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Depressed (No)</td>\n",
       "      <td>0.966154</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>0.973643</td>\n",
       "      <td>640.0</td>\n",
       "      <td>0.949254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Depressed (Yes)</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.949254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Macro Avg</td>\n",
       "      <td>0.683077</td>\n",
       "      <td>0.623958</td>\n",
       "      <td>0.646822</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.949254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Weighted Avg</td>\n",
       "      <td>0.940804</td>\n",
       "      <td>0.949254</td>\n",
       "      <td>0.944376</td>\n",
       "      <td>670.0</td>\n",
       "      <td>0.949254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model         grouping  precision    recall  f1-score  \\\n",
       "0  Random Forest Classifier   Depressed (No)   0.966154  0.981250  0.973643   \n",
       "1  Random Forest Classifier  Depressed (Yes)   0.400000  0.266667  0.320000   \n",
       "2  Random Forest Classifier        Macro Avg   0.683077  0.623958  0.646822   \n",
       "3  Random Forest Classifier     Weighted Avg   0.940804  0.949254  0.944376   \n",
       "\n",
       "   support  accuracy  \n",
       "0    640.0  0.949254  \n",
       "1     30.0  0.949254  \n",
       "2    670.0  0.949254  \n",
       "3    670.0  0.949254  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_x_train, rf_x_test, rf_y_train, rf_y_test = get_model_data(\n",
    "    original_df = cdc_survey_pmom,\n",
    "    columns = rf_model_features\n",
    ")\n",
    "\n",
    "rus = RandomUnderSampler(\n",
    "    random_state=42, \n",
    "    sampling_strategy=0.12,\n",
    "    replacement=False\n",
    ")\n",
    "rf_x_train_rus, rf_y_train_rus = rus.fit_resample(rf_x_train,rf_y_train)\n",
    "\n",
    "print(rf_x_train.shape)\n",
    "print(rf_x_test.shape)\n",
    "print(rf_y_train.shape)\n",
    "print(rf_y_test.shape)\n",
    "print(rf_x_train_rus.shape)\n",
    "print(rf_y_train_rus.shape)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(rf_x_train_rus, rf_y_train_rus)\n",
    "rf_pred = rf.predict(rf_x_test)\n",
    "rf_score = get_performance_df(rf_y_test, rf_pred,'Random Forest Classifier')\n",
    "rf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
       " 'max_features': ['auto', 'sqrt', 'log2', None],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = list(range(10,110, 10))\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = list(range(10,120, 10))\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "rf_param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid=rf_param_grid\n",
    ")\n",
    "rf_grid_search.fit(rf_x_train_rus, rf_y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=3, max_leaf_nodes=9, n_estimators=25)\n"
     ]
    }
   ],
   "source": [
    "print(rf_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_x_train_rus.shape)\n",
    "\n",
    "rf_hyper = RandomForestClassifier(\n",
    "    n_estimators=,\n",
    "    max_features=,\n",
    "    max_depth=,\n",
    "    min_samples_split=,\n",
    "    min_samples_leaf=,\n",
    "    bootstrap=,\n",
    "    random_state=42\n",
    ")\n",
    "rf_hyper.fit(rf_x_train_rus, rf_y_train_rus)\n",
    "rf_hyper_pred = rf_hyper.predict(rf_x_test)\n",
    "rf_hyper_score = get_performance_df(rf_y_test, rf_hyper_pred,'Random Forest Classifier')\n",
    "rf_hyper_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
