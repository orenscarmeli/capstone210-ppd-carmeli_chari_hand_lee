{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(df_cdc_clean, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, pred_labels):\n",
    "    \"\"\"\n",
    "    Function that displays a confusion matrix for provided true and predicted classes\n",
    "    \"\"\"\n",
    "    #print(f'cover type 1 and type 2 total correct {np.sum(np.diag(metrics.confusion_matrix(y_test, pred_labels))[:2])}')\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    disp = disp.plot(include_values=True, cmap='viridis', ax=ax, xticks_rotation='horizontal')    \n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_performance_df(label_actual, label_pred, index_label):\n",
    "    \"\"\"\n",
    "    Function to calculate performance metrics for model.\n",
    "    Includes precision, recal, F1, & support.\n",
    "    \"\"\"\n",
    "    performance_report = classification_report(label_actual, label_pred, output_dict=True)\n",
    "    performance_report = performance_report['macro avg']\n",
    "    result_table = pd.DataFrame(performance_report, index = [index_label])\n",
    "    return result_table\n",
    "\n",
    "def baseline_models(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    do_smote=True,\n",
    "    show_confusion_matrix=False,\n",
    "    show_score_dataframe=False):\n",
    "    \"\"\"\n",
    "    Function that trains and makes predictions using 5 of the classifiers went over during the class.\n",
    "    Meant as a helper function for easier testing of different modeling pipelines.\n",
    "    \"\"\"\n",
    "\n",
    "    #  do_smote\n",
    "    if do_smote == True:\n",
    "        # have to impute first because smote won't take nulls\n",
    "        my_imputer = SimpleImputer()\n",
    "        X_train = my_imputer.fit_transform(X_train)\n",
    "        X_test = my_imputer.fit_transform(X_test)\n",
    "\n",
    "        #print(X_train.shape)\n",
    "        #print(X_test.shape)\n",
    "\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "        # after smote oversampling\n",
    "        #print(X_train.shape)\n",
    "        \n",
    "    # SimpleImputer() = fill in missing values \n",
    "    # RobustScaler() = scale features to remove outliers\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    knn = make_pipeline(SimpleImputer(), RobustScaler(), KNeighborsClassifier())\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_labels_knn  = knn.predict(X_test)\n",
    "    #score_knn = recall_score(y_test, pred_labels_knn, average='macro')\n",
    "    score_knn = get_performance_df(y_test, pred_labels_knn,'Knn')\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lm = make_pipeline(SimpleImputer(), RobustScaler(), LogisticRegression()) \n",
    "    lm.fit(X_train, y_train)\n",
    "    pred_labels_lr  = lm.predict(X_test)\n",
    "    #score_lr = recall_score(y_test, pred_labels_lr, average='macro')\n",
    "    score_lr = get_performance_df(y_test, pred_labels_lr,'Logistic Regression')\n",
    "        \n",
    "    # Bernoulii Naive Bayes\n",
    "    bnb = make_pipeline(SimpleImputer(), RobustScaler(), BernoulliNB())  \n",
    "    bnb.fit(X_train, y_train)\n",
    "    pred_labels_bnb  = bnb.predict(X_test)\n",
    "    #score_bnb = recall_score(y_test, pred_labels_bnb, average='macro')\n",
    "    score_bnb = get_performance_df(y_test, pred_labels_bnb,'Bernoulli Naive Bayes')    \n",
    "        \n",
    "    # Gaussian Naive Bayes\n",
    "    gnb = make_pipeline(SimpleImputer(), RobustScaler(), GaussianNB())\n",
    "    gnb.fit(X_train, y_train)\n",
    "    pred_labels_gnb  = gnb.predict(X_test)\n",
    "    # score_gnb = gnb.score(X_test, y_test)\n",
    "    score_gnb = get_performance_df(y_test, pred_labels_gnb,'Gaussian Naive Bayes')    \n",
    "\n",
    "    # Random Forest\n",
    "    rf = make_pipeline(SimpleImputer(), RobustScaler(), RandomForestClassifier(random_state=0))\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred_labels_rf  = rf.predict(X_test)\n",
    "    predictions_posterior_rf = rf.predict_proba(X_test)\n",
    "    #score_rf = recall_score(y_test, pred_labels_rf, average='macro')\n",
    "    score_rf = get_performance_df(y_test, pred_labels_rf,'Random Forest')      \n",
    "\n",
    "    # make dataframe with scores\n",
    "    scores = pd.concat([score_knn, score_lr, score_bnb, score_gnb, score_rf])\n",
    "    scores = scores.sort_values(by = 'recall', ascending=False)\n",
    "    \n",
    "    if show_score_dataframe:\n",
    "        display(scores.style.set_table_attributes('style=\"font-size: 17px\"').hide_index())\n",
    "    \n",
    "    if show_confusion_matrix:\n",
    "        print('\\nK-Nearest Neighbors Confusion Matrix')\n",
    "        plot_confusion_matrix(y_test, pred_labels_knn)\n",
    "        print('Logistic Regression Confusion Matrix')\n",
    "        plot_confusion_matrix(y_test, pred_labels_lr)\n",
    "        print('Bernoulli Naive Bayes Confusion Matrix')\n",
    "        plot_confusion_matrix(y_test, pred_labels_bnb)\n",
    "        print('Gaussian Naive Bayes Confusion Matrix')\n",
    "        plot_confusion_matrix(y_test, pred_labels_gnb)\n",
    "        print('Random Forest Confusion Matrix')\n",
    "        plot_confusion_matrix(y_test, pred_labels_rf)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperprameter grid search\n",
    "SVG is terrible, but need to do some gridsearch on above baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Pipeline\n",
    "processing_pipeline = make_pipeline(SimpleImputer(), RobustScaler(), SVR())\n",
    "\n",
    "params = {\n",
    "    \"simpleimputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"robustscaler__quantile_range\": [(25.0, 75.0), (30.0, 70.0)],\n",
    "    \"svr__C\": [0.1, 1.0],\n",
    "    \"svr__gamma\": [\"auto\", 0.1],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(processing_pipeline, param_grid=params, n_jobs=-1, cv=5, verbose=3)\n",
    "\n",
    "model_filename = \"model_pipeline.pkl\"\n",
    "model_path = join(getcwd(), model_filename)\n",
    "print(model_path)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Train R^2 Score : {grid.best_estimator_.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test R^2 Score : {grid.best_estimator_.score(X_test, y_test):.3f}\")\n",
    "print(f\"Best R^2 Score Through Grid Search : {grid.best_score_:.3f}\")\n",
    "print(f\"Best Parameters : {grid.best_params_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
