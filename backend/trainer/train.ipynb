{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mod\n",
    "from os import getcwd\n",
    "from os.path import exists, join\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import random\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import pickle\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, KBinsDiscretizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depression screener\n",
    "dep_screener_cols = [\n",
    "    'little_interest_in_doing_things',\n",
    "    'feeling_down_depressed_hopeless',\n",
    "    'trouble_falling_or_staying_asleep',\n",
    "    'feeling_tired_or_having_little_energy',\n",
    "    'poor_appetitie_or_overeating',\n",
    "    'feeling_bad_about_yourself',\n",
    "    'trouble_concentrating',\n",
    "    'moving_or_speaking_to_slowly_or_fast',\n",
    "    'thoughts_you_would_be_better_off_dead',\n",
    "    'difficult_doing_daytoday_tasks'\n",
    "]\n",
    "model_features_opt2 = dep_screener_cols + [\n",
    "    'seen_mental_health_professional',\n",
    "    'times_with_12plus_alc',\n",
    "    'time_since_last_healthcare',\n",
    "    'cholesterol_prescription',\n",
    "    'high_cholesterol',\n",
    "    'age_in_years',\n",
    "    'horomones_not_bc',\n",
    "    'months_since_birth',\n",
    "    'arthritis',\n",
    "    'high_bp',\n",
    "    'regular_periods',\n",
    "    'moderate_recreation',\n",
    "    'thyroid_issues',\n",
    "    'vigorous_recreation',\n",
    "    'stroke',\n",
    "    'is_usa_born',\n",
    "    'asthma',\n",
    "    'count_days_moderate_recreational_activity',\n",
    "    'have_health_insurance',\n",
    "    'num_dep_screener_0',\n",
    "    'weight_lbs_over_height_in_ratio'\n",
    "]\n",
    "\n",
    "model_features_low_opt7 = [\n",
    "    'count_days_seen_doctor_12mo_bin',\n",
    "    'times_with_12plus_alc',\n",
    "    'seen_mental_health_professional',\n",
    "    'count_lost_10plus_pounds',\n",
    "    'arthritis',\n",
    "    'horomones_not_bc',\n",
    "    'is_usa_born',\n",
    "    'times_with_8plus_alc',\n",
    "    'time_since_last_healthcare',\n",
    "    'duration_last_healthcare_visit',\n",
    "    'work_schedule'\n",
    "]\n",
    "\n",
    "columns_to_use_low = model_features_low_opt7\n",
    "columns_to_use_high = model_features_opt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opt 9: Ensemble Model\n",
    "\n",
    "Build 2 models with different feature set\n",
    "- Model 1: \n",
    " - GB trained on observations with 1+ dep screener response. \n",
    " - Uses features from opt 2. \n",
    " - Uses undersampler.\n",
    "- Model 2: \n",
    " - RF trained on observations with 0 dep screener response. \n",
    " - Uses features from opt 7. \n",
    " - Uses undersampler\n",
    "\n",
    "Notes\n",
    "- _low = has 9+ dep screeners answered 0\n",
    "- _high = has <9 dep screeners answered 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0k/kvbyl6wn7kl2qbt7_94pckjw0000gn/T/ipykernel_42108/3063162359.py:2: DtypeWarning: Columns (664,665) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_cdc_clean = pd.read_csv('../../data/cdc_nhanes_survey_responses_clean.csv')\n",
      "/var/folders/0k/kvbyl6wn7kl2qbt7_94pckjw0000gn/T/ipykernel_42108/3063162359.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cdc_survey_pmom['num_dep_screener_0'] = (cdc_survey_pmom[dep_screener_cols]==0).sum(axis=1)\n",
      "/var/folders/0k/kvbyl6wn7kl2qbt7_94pckjw0000gn/T/ipykernel_42108/3063162359.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cdc_survey_pmom['weight_lbs_over_height_in_ratio'] = round(cdc_survey_pmom['weight_lbs'] / cdc_survey_pmom['height_in'],1)\n",
      "/var/folders/0k/kvbyl6wn7kl2qbt7_94pckjw0000gn/T/ipykernel_42108/3063162359.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cdc_survey_pmom['count_days_seen_doctor_12mo_bin'] = fill_arr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7741, 863)\n"
     ]
    }
   ],
   "source": [
    "# v2 csv\n",
    "df_cdc_clean = pd.read_csv('../../data/cdc_nhanes_survey_responses_clean.csv')\n",
    "\n",
    "# filter to pregnant moms\n",
    "cdc_survey_pmom = df_cdc_clean[df_cdc_clean['has_been_pregnant'] == 1]\n",
    "print(cdc_survey_pmom.shape)\n",
    "## ##################################################################\n",
    "## Dep screener cnt\n",
    "\n",
    "cdc_survey_pmom['num_dep_screener_0'] = (cdc_survey_pmom[dep_screener_cols]==0).sum(axis=1)\n",
    "cdc_survey_pmom['weight_lbs_over_height_in_ratio'] = round(cdc_survey_pmom['weight_lbs'] / cdc_survey_pmom['height_in'],1)\n",
    "\n",
    "# add bins\n",
    "\n",
    "feature_values = cdc_survey_pmom['count_days_seen_doctor_12mo'].dropna().values\n",
    "feature_values = feature_values.reshape([feature_values.shape[0],1])\n",
    "\n",
    "# create bins using estimator\n",
    "est = KBinsDiscretizer(\n",
    "    n_bins=10,\n",
    "    encode='ordinal', \n",
    "    strategy='uniform', \n",
    "    subsample=None\n",
    ")\n",
    "est.fit(feature_values)\n",
    "feature_values = est.transform(feature_values)\n",
    "\n",
    "fill_arr = cdc_survey_pmom['count_days_seen_doctor_12mo'].values.copy()\n",
    "fill_arr[~np.isnan(fill_arr)] = np.asarray([val[0] for val in feature_values])\n",
    "cdc_survey_pmom['count_days_seen_doctor_12mo_bin'] = fill_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# subset to features and do preprocessing\n",
    "imputer = SimpleImputer(strategy='median')  \n",
    "trans = RobustScaler()\n",
    "\n",
    "\n",
    "data_low_dep_screener = cdc_survey_pmom[cdc_survey_pmom['num_dep_screener_0'] >= 9].copy()\n",
    "data_low_dep_screener = data_low_dep_screener[['MDD'] + columns_to_use_low]\n",
    "\n",
    "y_low = data_low_dep_screener['MDD'].values\n",
    "x_low = data_low_dep_screener.iloc[:,1:].values\n",
    "x_low = imputer.fit_transform(x_low)\n",
    "x_low = trans.fit_transform(x_low)\n",
    "x_train_low, x_test_low, y_train_low, y_test_low = train_test_split(\n",
    "    x_low, \n",
    "    y_low, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ") \n",
    "\n",
    "data_high_dep_screener = cdc_survey_pmom[cdc_survey_pmom['num_dep_screener_0'] < 9].copy()\n",
    "data_high_dep_screener = data_high_dep_screener[['MDD'] + columns_to_use_high]\n",
    "y_high = data_high_dep_screener['MDD'].values\n",
    "x_high = data_high_dep_screener.iloc[:,1:].values\n",
    "x_high = imputer.fit_transform(x_high)\n",
    "x_high = trans.fit_transform(x_high)\n",
    "x_train_high, x_test_high, y_train_high, y_test_high = train_test_split(\n",
    "    x_high, \n",
    "    y_high, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBin(TransformerMixin):\n",
    "    def __init__(self, some_stuff=None, column_names= []):\n",
    "        pass\n",
    "        # self.some_stuff = some_stuff\n",
    "        self.column_names = column_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # do stuff on X, and return dataframe\n",
    "        # of the same shape - this gets messy\n",
    "        # if the preceding item is a numpy array\n",
    "        # and not a dataframe\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.column_names)\n",
    "        feature_values = X['count_days_seen_doctor_12mo'].dropna().values\n",
    "        feature_values = feature_values.reshape([feature_values.shape[0],1])\n",
    "\n",
    "        # create bins using estimator\n",
    "        est = KBinsDiscretizer(\n",
    "            n_bins=10,\n",
    "            encode='ordinal', \n",
    "            strategy='uniform', \n",
    "            subsample=None\n",
    "        )\n",
    "        est.fit(feature_values)\n",
    "        feature_values = est.transform(feature_values)\n",
    "\n",
    "        fill_arr = X['count_days_seen_doctor_12mo'].values.copy()\n",
    "        fill_arr[~np.isnan(fill_arr)] = np.asarray([val[0] for val in feature_values])\n",
    "        X['count_days_seen_doctor_12mo_bin'] = fill_arr\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# # using this by itself works as well\n",
    "# my_pipeline = make_pipeline(CustomBin(column_names=[\"my_str\", \"val\"]))\n",
    "# my_pipeline.fit_transform(cdc_survey_pmom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomFeatures(TransformerMixin):\n",
    "    def __init__(self, some_stuff=None, column_names= []):\n",
    "        pass\n",
    "        # self.some_stuff = some_stuff\n",
    "        self.column_names = column_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # do stuff on X, and return dataframe\n",
    "        # of the same shape - this gets messy\n",
    "        # if the preceding item is a numpy array\n",
    "        # and not a dataframe\n",
    "        if isinstance(X, np.ndarray):\n",
    "            X = pd.DataFrame(X, columns=self.column_names)\n",
    "        X['num_dep_screener_0'] = (X[dep_screener_cols]==0).sum(axis=1)\n",
    "        X['weight_lbs_over_height_in_ratio'] = round(X['weight_lbs'] / X['height_in'],1)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# # using this by itself works as well\n",
    "# my_pipeline = make_pipeline(CustomFeatures(column_names=[\"my_str\", \"val\"]))\n",
    "# my_pipeline.fit_transform(cdc_survey_pmom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_high_rus: (982, 31)\n",
      "x_train_low_rus: (849, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.956664</td>\n",
       "      <td>0.358896</td>\n",
       "      <td>0.830859</td>\n",
       "      <td>0.657780</td>\n",
       "      <td>0.891060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.848441</td>\n",
       "      <td>0.688235</td>\n",
       "      <td>0.830859</td>\n",
       "      <td>0.768338</td>\n",
       "      <td>0.830859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.899308</td>\n",
       "      <td>0.471774</td>\n",
       "      <td>0.830859</td>\n",
       "      <td>0.685541</td>\n",
       "      <td>0.852387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>1379.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>0.830859</td>\n",
       "      <td>1549.000000</td>\n",
       "      <td>1549.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0           1  accuracy    macro avg  weighted avg\n",
       "precision     0.956664    0.358896  0.830859     0.657780      0.891060\n",
       "recall        0.848441    0.688235  0.830859     0.768338      0.830859\n",
       "f1-score      0.899308    0.471774  0.830859     0.685541      0.852387\n",
       "support    1379.000000  170.000000  0.830859  1549.000000   1549.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## #####################################################\n",
    "## Model 1: Based on observations with 1+ dep screener non-zero response\n",
    "\n",
    "# partially correct for class imbalance\n",
    "rus_model1 = RandomUnderSampler(\n",
    "    random_state=42, \n",
    "    sampling_strategy=1,\n",
    "    replacement=False\n",
    ")\n",
    "x_train_high_rus, y_train_high_rus = rus_model1.fit_resample(x_train_high,y_train_high)\n",
    "print(f\"x_train_high_rus: {x_train_high_rus.shape}\")\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(x_train_high_rus, y_train_high_rus)\n",
    "y_pred_high = gb.predict(x_test_high)\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(x_train_high_rus, y_train_high_rus)\n",
    "y_pred_high = gb.predict(x_test_high)\n",
    "\n",
    "## #####################################################\n",
    "## Model 1: Based on observations with 0 dep screener non-zero response\n",
    "\n",
    "# partially correct for class imbalance\n",
    "rus = RandomUnderSampler(\n",
    "    random_state=42, \n",
    "    sampling_strategy=0.12,\n",
    "    replacement=False\n",
    ")\n",
    "x_train_low_rus, y_train_low_rus = rus.fit_resample(x_train_low,y_train_low)\n",
    "print(f\"x_train_low_rus: {x_train_low_rus.shape}\")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(x_train_low_rus, y_train_low_rus)\n",
    "y_pred_low = rf.predict(x_test_low)\n",
    "\n",
    "## #####################################################\n",
    "## Predict: \n",
    "\n",
    "y_pred = np.concatenate((y_pred_high, y_pred_low))\n",
    "y_actual = np.concatenate((y_test_high, y_test_low))\n",
    "\n",
    "pd.DataFrame(classification_report(y_actual,y_pred,output_dict=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This RandomForestClassifier estimator requires y to be passed, but the target y is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 36\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# processing_pipeline = make_pipeline(\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#     SimpleImputer(strategy='most_frequent'),\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#     RobustScaler(), \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#     GradientBoostingClassifier(random_state=42)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     21\u001b[0m processing_pipeline \u001b[39m=\u001b[39m make_pipeline(\n\u001b[1;32m     22\u001b[0m     \u001b[39m# CustomFeatures(CustomBin(column_names=['MDD'] + columns_to_use_low)),\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39m# CustomBin(CustomBin(column_names=['MDD'] + columns_to_use_low)),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     RandomForestClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m processing_pipeline\u001b[39m.\u001b[39;49mfit(x_train_high_rus)\n\u001b[1;32m     37\u001b[0m \u001b[39m# pred_labels  = processing_pipeline.predict(x_test)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# pred_labels = [x.round() for x in pred_labels]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/w210_Capstone/capstone210-ppd-carmeli_chari_hand_lee/.venv/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/w210_Capstone/capstone210-ppd-carmeli_chari_hand_lee/.venv/lib/python3.10/site-packages/sklearn/pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 420\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/w210_Capstone/capstone210-ppd-carmeli_chari_hand_lee/.venv/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/w210_Capstone/capstone210-ppd-carmeli_chari_hand_lee/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    347\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    349\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    350\u001b[0m )\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/Desktop/w210_Capstone/capstone210-ppd-carmeli_chari_hand_lee/.venv/lib/python3.10/site-packages/sklearn/base.py:582\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_feature_names(X, reset\u001b[39m=\u001b[39mreset)\n\u001b[1;32m    581\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 582\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    583\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    587\u001b[0m no_val_X \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(X, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m X \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m no_val_y \u001b[39m=\u001b[39m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(y, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m y \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This RandomForestClassifier estimator requires y to be passed, but the target y is None."
     ]
    }
   ],
   "source": [
    "\n",
    "data_low_dep_screener = cdc_survey_pmom[cdc_survey_pmom['num_dep_screener_0'] >= 9].copy()\n",
    "data_low_dep_screener = data_low_dep_screener[['MDD'] + columns_to_use_low]\n",
    "\n",
    "y_low = data_low_dep_screener['MDD'].values\n",
    "x_low = data_low_dep_screener.iloc[:,1:].values\n",
    "x_low = imputer.fit_transform(x_low)\n",
    "x_low = trans.fit_transform(x_low)\n",
    "x_train_low, x_test_low, y_train_low, y_test_low = train_test_split(\n",
    "    x_low, \n",
    "    y_low, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ") \n",
    "\n",
    "rus_model1 = RandomUnderSampler(\n",
    "    random_state=42, \n",
    "    sampling_strategy=1,\n",
    "    replacement=False\n",
    ")\n",
    "x_train_high_rus, y_train_high_rus = rus_model1.fit_resample(x_train_high,y_train_high)\n",
    "print(f\"x_train_high_rus: {x_train_high_rus.shape}\")\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(x_train_high_rus, y_train_high_rus)\n",
    "y_pred_high = gb.predict(x_test_high)\n",
    "\n",
    "\n",
    "# processing_pipeline = make_pipeline(\n",
    "#     SimpleImputer(strategy='most_frequent'),\n",
    "#     RobustScaler(), \n",
    "#     GradientBoostingClassifier(random_state=42)\n",
    "# )\n",
    "\n",
    "processing_pipeline = make_pipeline(\n",
    "    CustomFeatures(CustomBin(column_names=['MDD'] + columns_to_use_low)),\n",
    "    CustomBin(CustomBin(column_names=['MDD'] + columns_to_use_low)),\n",
    "    SimpleImputer(strategy='median') , \n",
    "    RobustScaler(),\n",
    "    # RandomUnderSampler(\n",
    "    #     random_state=42, \n",
    "    #     sampling_strategy=0.12,\n",
    "    #     replacement=False\n",
    "    # ), \n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "processing_pipeline.fit(x_train_high_rus)\n",
    "# pred_labels  = processing_pipeline.predict(x_test)\n",
    "# pred_labels = [x.round() for x in pred_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), RobustScaler(), GradientBoostingClassifier(random_state=42))\n",
    "\n",
    "model_filename = \"model_pipeline.pkl\"\n",
    "model_path = join(getcwd(), model_filename)\n",
    "processing_pipeline.fit(x_train, y_train)\n",
    "pred_labels  = processing_pipeline.predict(x_test)\n",
    "pred_labels = [x.round() for x in pred_labels]\n",
    "df_gnb = pd.DataFrame(classification_report(\n",
    "    y_test, \n",
    "    pred_labels, \n",
    "    # target_names=target_names, \n",
    "    output_dict=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols_to_keep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m get_model_data(\n\u001b[1;32m      2\u001b[0m     original_df \u001b[39m=\u001b[39m df_cdc_clean,\n\u001b[0;32m----> 3\u001b[0m     columns \u001b[39m=\u001b[39m cols_to_keep\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m processing_pipeline \u001b[39m=\u001b[39m make_pipeline(SimpleImputer(strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmost_frequent\u001b[39m\u001b[39m'\u001b[39m), RobustScaler(), GaussianNB())\n\u001b[1;32m      8\u001b[0m model_filename \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_pipeline.pkl\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cols_to_keep' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = get_model_data(\n",
    "    original_df = df_cdc_clean,\n",
    "    columns = cols_to_keep\n",
    ")\n",
    "\n",
    "processing_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), RobustScaler(), GaussianNB())\n",
    "\n",
    "model_filename = \"model_pipeline.pkl\"\n",
    "model_path = join(getcwd(), model_filename)\n",
    "if not exists(model_path):\n",
    "    processing_pipeline.fit(x_train, y_train)\n",
    "    pred_labels  = processing_pipeline.predict(x_test)\n",
    "    pred_labels = [x.round() for x in pred_labels]\n",
    "    df_gnb = pd.DataFrame(classification_report(\n",
    "                y_test, \n",
    "                pred_labels, \n",
    "                # target_names=target_names, \n",
    "                output_dict=True\n",
    "    ))\n",
    "    joblib.dump(processing_pipeline, model_path)\n",
    "    print('successfully trained model')\n",
    "else:\n",
    "    print(\"Model has already been trained, no need to rerun\")\n",
    "\n",
    "df_gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: trainer.py: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    !rm trainer.py\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook trainer.ipynb to script\n",
      "[NbConvertApp] Writing 26351 bytes to trainer.py\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    !jupyter nbconvert --no-prompt --to script trainer.ipynb\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
