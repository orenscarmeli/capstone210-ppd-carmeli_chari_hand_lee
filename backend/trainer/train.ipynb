{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mod\n",
    "from os import getcwd\n",
    "from os.path import exists, join\n",
    "\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import random\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "import pickle\n",
    "from IPython.display import Image\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler, KBinsDiscretizer\n",
    "\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depression screener\n",
    "dep_screener_cols = [\n",
    "    'little_interest_in_doing_things',\n",
    "    'feeling_down_depressed_hopeless',\n",
    "    'trouble_falling_or_staying_asleep',\n",
    "    'feeling_tired_or_having_little_energy',\n",
    "    'poor_appetitie_or_overeating',\n",
    "    'feeling_bad_about_yourself',\n",
    "    'trouble_concentrating',\n",
    "    'moving_or_speaking_to_slowly_or_fast',\n",
    "    'thoughts_you_would_be_better_off_dead',\n",
    "    'difficult_doing_daytoday_tasks'\n",
    "]\n",
    "model_features_opt2 = dep_screener_cols + [\n",
    "    'seen_mental_health_professional',\n",
    "    'times_with_12plus_alc',\n",
    "    'time_since_last_healthcare',\n",
    "    'cholesterol_prescription',\n",
    "    'high_cholesterol',\n",
    "    'age_in_years',\n",
    "    'horomones_not_bc',\n",
    "    'months_since_birth',\n",
    "    'arthritis',\n",
    "    'high_bp',\n",
    "    'regular_periods',\n",
    "    'moderate_recreation',\n",
    "    'thyroid_issues',\n",
    "    'vigorous_recreation',\n",
    "    'stroke',\n",
    "    'is_usa_born',\n",
    "    'asthma',\n",
    "    'count_days_moderate_recreational_activity',\n",
    "    'have_health_insurance',\n",
    "    'num_dep_screener_0',\n",
    "    'weight_lbs_over_height_in_ratio'\n",
    "]\n",
    "\n",
    "model_features_low_opt7 = [\n",
    "    'count_days_seen_doctor_12mo_bin',\n",
    "    'times_with_12plus_alc',\n",
    "    'seen_mental_health_professional',\n",
    "    'count_lost_10plus_pounds',\n",
    "    'arthritis',\n",
    "    'horomones_not_bc',\n",
    "    'is_usa_born',\n",
    "    'times_with_8plus_alc',\n",
    "    'time_since_last_healthcare',\n",
    "    'duration_last_healthcare_visit',\n",
    "    'work_schedule'\n",
    "]\n",
    "\n",
    "columns_to_use_low = model_features_low_opt7\n",
    "columns_to_use_high = model_features_opt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opt 9: Ensemble Model\n",
    "\n",
    "Build 2 models with different feature set\n",
    "- Model 1: \n",
    " - GB trained on observations with 1+ dep screener response. \n",
    " - Uses features from opt 2. \n",
    " - Uses undersampler.\n",
    "- Model 2: \n",
    " - RF trained on observations with 0 dep screener response. \n",
    " - Uses features from opt 7. \n",
    " - Uses undersampler\n",
    "\n",
    "Notes\n",
    "- _low = has 9+ dep screeners answered 0\n",
    "- _high = has <9 dep screeners answered 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class CustomFeatures(TransformerMixin):\n",
    "#     def __init__(self, some_stuff=None, column_names= []):\n",
    "#         pass\n",
    "#         # self.some_stuff = some_stuff\n",
    "#         self.column_names = column_names\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         # do stuff on X, and return dataframe\n",
    "#         # of the same shape - this gets messy\n",
    "#         # if the preceding item is a numpy array\n",
    "#         # and not a dataframe\n",
    "#         if isinstance(X, np.ndarray):\n",
    "#             X = pd.DataFrame(X)\n",
    "#         X['num_dep_screener_0'] = (X[dep_screener_cols]==0).sum(axis=1)\n",
    "#         X['weight_lbs_over_height_in_ratio'] = round(X['weight_lbs'] / X['height_in'],1)\n",
    "\n",
    "#         return X\n",
    "\n",
    "\n",
    "# # # using this by itself works as well\n",
    "# # my_pipeline = make_pipeline(CustomFeatures(column_names=[\"my_str\", \"val\"]))\n",
    "# # my_pipeline.fit_transform(cdc_survey_pmom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBin(TransformerMixin):\n",
    "    def __init__(self, some_stuff=None, column_names= []):\n",
    "        pass\n",
    "        # self.some_stuff = some_stuff\n",
    "        self.column_names = column_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # do stuff on X, and return dataframe\n",
    "        # of the same shape - this gets messy\n",
    "        # if the preceding item is a numpy array\n",
    "        # and not a dataframe\n",
    "        # if isinstance(X, np.ndarray):\n",
    "        #     X = pd.DataFrame(X, columns=self.column_names)\n",
    "        feature_values = X.dropna().values\n",
    "        feature_values = feature_values.reshape([feature_values.shape[0],1])\n",
    "\n",
    "        # create bins using estimator\n",
    "        est = KBinsDiscretizer(\n",
    "            n_bins=10,\n",
    "            encode='ordinal', \n",
    "            strategy='uniform', \n",
    "            subsample=None\n",
    "        )\n",
    "        est.fit(feature_values)\n",
    "        feature_values = est.transform(feature_values)\n",
    "\n",
    "        fill_arr = X['count_days_seen_doctor_12mo'].values.copy()\n",
    "        fill_arr[~np.isnan(fill_arr)] = np.asarray([val[0] for val in feature_values])\n",
    "        X['count_days_seen_doctor_12mo_bin'] = fill_arr\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "# # using this by itself works as well\n",
    "# my_pipeline = make_pipeline(CustomBin(column_names=[\"my_str\", \"val\"]))\n",
    "# my_pipeline.fit_transform(cdc_survey_pmom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 csv\n",
    "df_cdc_clean = pd.read_csv('../../data/cdc_nhanes_survey_responses_clean.csv')\n",
    "\n",
    "# filter to pregnant moms\n",
    "cdc_survey_pmom = df_cdc_clean[df_cdc_clean['has_been_pregnant'] == 1]\n",
    "print(cdc_survey_pmom.shape)\n",
    "\n",
    "# add features\n",
    "cdc_survey_pmom['num_dep_screener_0'] = (cdc_survey_pmom[dep_screener_cols]==0).sum(axis=1)\n",
    "cdc_survey_pmom['weight_lbs_over_height_in_ratio'] = round(df_cdc_clean['weight_lbs'] / cdc_survey_pmom['height_in'],1)\n",
    "\n",
    "feature_values = cdc_survey_pmom['count_days_seen_doctor_12mo'].dropna().values\n",
    "feature_values = feature_values.reshape([feature_values.shape[0],1])\n",
    "\n",
    "# create bins using estimator\n",
    "est = KBinsDiscretizer(\n",
    "    n_bins=10,\n",
    "    encode='ordinal', \n",
    "    strategy='uniform', \n",
    "    subsample=None\n",
    ")\n",
    "est.fit(feature_values)\n",
    "\n",
    "# dump kbins so we can use this on inference\n",
    "model_filename = \"model_kbins.pkl\"\n",
    "model_path = join(getcwd(), model_filename)\n",
    "joblib.dump(est, model_path)\n",
    "\n",
    "feature_values = est.transform(feature_values)\n",
    "\n",
    "fill_arr = cdc_survey_pmom['count_days_seen_doctor_12mo'].values.copy()\n",
    "fill_arr[~np.isnan(fill_arr)] = np.asarray([val[0] for val in feature_values])\n",
    "cdc_survey_pmom['count_days_seen_doctor_12mo_bin'] = fill_arr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.bin_edges_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to features and do preprocessing\n",
    "data_low_dep_screener = cdc_survey_pmom[cdc_survey_pmom['num_dep_screener_0'] >= 9].copy()\n",
    "data_low_dep_screener = data_low_dep_screener[['MDD'] + columns_to_use_low]\n",
    "y_low = data_low_dep_screener['MDD'].values\n",
    "x_low = data_low_dep_screener.iloc[:,1:].values\n",
    "\n",
    "x_train_low, x_test_low, y_train_low, y_test_low = train_test_split(\n",
    "    x_low, \n",
    "    y_low, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# impute and scale\n",
    "imputer_low = SimpleImputer(strategy='median')  \n",
    "trans_low = RobustScaler()\n",
    "x_train_low = imputer_low.fit_transform(x_train_low)\n",
    "x_train_low = trans_low.fit_transform(x_train_low)\n",
    "\n",
    "x_test_low = imputer_low.fit_transform(x_test_low)\n",
    "x_test_low = trans_low.fit_transform(x_test_low)\n",
    "\n",
    "# partially correct for class imbalance\n",
    "rus = RandomUnderSampler(\n",
    "    random_state=42, \n",
    "    sampling_strategy=0.12,\n",
    "    replacement=False\n",
    ")\n",
    "x_train_low_rus, y_train_low_rus = rus.fit_resample(x_train_low,y_train_low)\n",
    "print(f\"x_train_low_rus: {x_train_low_rus.shape}\")\n",
    "\n",
    "\n",
    "# fit\n",
    "# gb_1 = GradientBoostingClassifier(random_state=42)\n",
    "# gb_1.fit(x_train_low_rus, y_train_low_rus)\n",
    "# y_pred_low = gb_1.predict(x_test_low)\n",
    "\n",
    "low_pipeline = make_pipeline(imputer_low, trans_low, \n",
    "    GradientBoostingClassifier(\n",
    "        random_state=42)\n",
    ")\n",
    "low_pipeline[:2].fit(x_train_low_rus)\n",
    "low_pipeline[2].fit(x_train_low_rus, y_train_low_rus)\n",
    "\n",
    "y_pred_low = low_pipeline.predict(x_train_low_rus)\n",
    "# to pickle for inference later\n",
    "\n",
    "model_filename = \"model_low_imp_scl.pkl\"\n",
    "model_path = join(getcwd(), model_filename)\n",
    "joblib.dump(low_pipeline, model_path)\n",
    "\n",
    "\n",
    "pd.DataFrame(classification_report(y_train_low_rus,y_pred_low,output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_filename = \"model_pipeline_low.pkl\"\n",
    "# model_path = join(getcwd(), model_filename)\n",
    "# joblib.dump(gb_1, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_high_dep_screener = cdc_survey_pmom[cdc_survey_pmom['num_dep_screener_0'] < 9].copy()\n",
    "data_high_dep_screener = data_high_dep_screener[['MDD'] + columns_to_use_high]\n",
    "y_high = data_high_dep_screener['MDD'].values\n",
    "x_high = data_high_dep_screener.iloc[:,1:].values\n",
    "\n",
    "x_train_high, x_test_high, y_train_high, y_test_high = train_test_split(\n",
    "    x_high, \n",
    "    y_high, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ") \n",
    "print(x_train_high.shape)\n",
    "# impute and scale\n",
    "imputer_high = SimpleImputer(strategy='median')  \n",
    "trans_high = RobustScaler()\n",
    "x_train_high = imputer_high.fit_transform(x_train_high)\n",
    "x_train_high = trans_high.fit_transform(x_train_high)\n",
    "\n",
    "x_test_high = imputer_high.fit_transform(x_test_high)\n",
    "x_test_high = trans_high.fit_transform(x_test_high)\n",
    "# partially correct for class imbalance\n",
    "rus_model1 = RandomUnderSampler(\n",
    "    random_state=42, \n",
    "    sampling_strategy=1,\n",
    "    replacement=False\n",
    ")\n",
    "x_train_high_rus, y_train_high_rus = rus_model1.fit_resample(x_train_high,y_train_high)\n",
    "print(f\"x_train_high_rus: {x_train_high_rus.shape}\")\n",
    "# gb = GradientBoostingClassifier(random_state=42)\n",
    "# gb.fit(x_train_high_rus, y_train_high_rus)\n",
    "# y_pred_high = gb.predict(x_test_high)\n",
    "\n",
    "# make pipeline version\n",
    "high_pipeline = make_pipeline(imputer_high, trans_high, GradientBoostingClassifier(random_state=42))\n",
    "high_pipeline[:2].fit(x_train_high_rus)\n",
    "high_pipeline[2].fit(x_train_high_rus, y_train_high_rus)\n",
    "\n",
    "y_pred_high = high_pipeline.predict(x_train_high_rus)\n",
    "# to pickle for inference later\n",
    "\n",
    "model_filename = \"model_high_imp_scl.pkl\"\n",
    "model_path = join(getcwd(), model_filename)\n",
    "joblib.dump(high_pipeline, model_path)\n",
    "\n",
    "pd.DataFrame(classification_report(y_train_high_rus,y_pred_high,output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_filename = \"model_pipeline_high.pkl\"\n",
    "# model_path = join(getcwd(), model_filename)\n",
    "# joblib.dump(gb, model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    !rm train.py\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train.ipynb to script\n",
      "[NbConvertApp] Writing 11565 bytes to train.py\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    !jupyter nbconvert --no-prompt --to script train.ipynb\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
